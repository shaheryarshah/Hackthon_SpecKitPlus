"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[769],{7166:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"humanoid-robotics/hri","title":"Human-Robot Interaction (HRI)","description":"Human-Robot Interaction (HRI) is a critical aspect of humanoid robotics that focuses on the design, evaluation, and implementation of robots that interact with humans. As humanoid robots become more prevalent in our daily lives, creating natural, intuitive, and safe interaction methods becomes increasingly important. HRI encompasses multiple modalities including verbal communication, gesture interpretation, facial expressions, and collaborative task execution.","source":"@site/docs/05-humanoid-robotics/hri.md","sourceDirName":"05-humanoid-robotics","slug":"/humanoid-robotics/hri","permalink":"/Hackthon_SpecKitPlus/docs/humanoid-robotics/hri","draft":false,"unlisted":false,"editUrl":"https://github.com/shaheryarshah/Hackthon_SpecKitPlus/edit/main/docs/docs/05-humanoid-robotics/hri.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation & Humanoid Hands","permalink":"/Hackthon_SpecKitPlus/docs/humanoid-robotics/manipulation"},"next":{"title":"Vision-Language-Action Robotics","permalink":"/Hackthon_SpecKitPlus/docs/vla-robotics/intro"}}');var s=t(4848),o=t(8453);const a={},r="Human-Robot Interaction (HRI)",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Social Robotics Principles",id:"social-robotics-principles",level:3},{value:"Multimodal Communication",id:"multimodal-communication",level:3},{value:"Collaborative Behaviors",id:"collaborative-behaviors",level:3},{value:"Equations and Models",id:"equations-and-models",level:2},{value:"Theory of Mind Model",id:"theory-of-mind-model",level:3},{value:"Proxemics Model",id:"proxemics-model",level:3},{value:"Interaction Quality Metric",id:"interaction-quality-metric",level:3},{value:"Code Example: Human-Robot Interaction System",id:"code-example-human-robot-interaction-system",level:2},{value:"Simulation Demonstration",id:"simulation-demonstration",level:2},{value:"Hands-On Lab: Human-Robot Interaction Implementation",id:"hands-on-lab-human-robot-interaction-implementation",level:2},{value:"Required Equipment:",id:"required-equipment",level:3},{value:"Instructions:",id:"instructions",level:3},{value:"Common Pitfalls &amp; Debugging Notes",id:"common-pitfalls--debugging-notes",level:2},{value:"Summary &amp; Key Terms",id:"summary--key-terms",level:2},{value:"Further Reading &amp; Citifications",id:"further-reading--citifications",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"human-robot-interaction-hri",children:"Human-Robot Interaction (HRI)"})}),"\n",(0,s.jsx)(n.p,{children:"Human-Robot Interaction (HRI) is a critical aspect of humanoid robotics that focuses on the design, evaluation, and implementation of robots that interact with humans. As humanoid robots become more prevalent in our daily lives, creating natural, intuitive, and safe interaction methods becomes increasingly important. HRI encompasses multiple modalities including verbal communication, gesture interpretation, facial expressions, and collaborative task execution."}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"After completing this section, you should be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Design intuitive interfaces for human-robot interaction"}),"\n",(0,s.jsx)(n.li,{children:"Implement multimodal interaction systems using voice, gesture, and vision"}),"\n",(0,s.jsx)(n.li,{children:"Create socially-aware robotic behaviors that consider human comfort and expectations"}),"\n",(0,s.jsx)(n.li,{children:"Assess and improve the usability of human-robot interfaces"}),"\n",(0,s.jsx)(n.li,{children:"Design and implement collaborative behaviors between humans and robots"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the effectiveness of HRI systems"}),"\n",(0,s.jsx)(n.li,{children:"Address safety and ethical considerations in HRI systems"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"social-robotics-principles",children:"Social Robotics Principles"}),"\n",(0,s.jsx)(n.p,{children:"Humanoid robots must exhibit behaviors that make humans comfortable and facilitate natural interaction:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social Presence"}),": The robot should appear aware and engaged with humans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Approachability"}),": Design that invites interaction and reduces intimidation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Predictability"}),": Robot behaviors should be understandable and anticipate-able"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reciprocity"}),": The robot should respond appropriately to human actions"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"multimodal-communication",children:"Multimodal Communication"}),"\n",(0,s.jsx)(n.p,{children:"Effective HRI systems integrate multiple communication channels:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verbal Communication"}),": Natural language processing for speech understanding and generation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gesture Recognition"}),": Understanding human gestures and producing meaningful robot gestures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Facial Expression"}),": Expressive capabilities that convey emotional states or intentions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Proxemics"}),": Understanding spatial relationships and personal space preferences"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"collaborative-behaviors",children:"Collaborative Behaviors"}),"\n",(0,s.jsx)(n.p,{children:"HRI systems should facilitate effective human-robot teamwork:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Joint Attention"}),": The ability for robot and human to focus on the same object or task"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Turn Taking"}),": Proper timing in conversations and collaborative activities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Prediction"}),": Understanding human intentions to provide anticipatory assistance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Role Negotiation"}),": Adapting to changing roles during interaction"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"equations-and-models",children:"Equations and Models"}),"\n",(0,s.jsx)(n.h3,{id:"theory-of-mind-model",children:"Theory of Mind Model"}),"\n",(0,s.jsx)(n.p,{children:"The robot's understanding of human mental states:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"P(M_h | O_r) = P(O_r | M_h) * P(M_h) / P(O_r)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"M_h"})," represents the mental state of the human (beliefs, intentions, desires)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"O_r"})," represents the robot's observations of the human"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"P(M_h | O_r)"})," is the probability of the human's mental state given the observations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"proxemics-model",children:"Proxemics Model"}),"\n",(0,s.jsx)(n.p,{children:"Hall's proxemics zones adapted for human-robot interaction:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Personal_Space_Radius = f(Relationship_Type, Context, Cultural_Factors)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where the robot maintains different distances based on social context:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Intimate zone (0-45 cm): Reserved for special interactions"}),"\n",(0,s.jsx)(n.li,{children:"Personal zone (45-120 cm): Normal conversation distance"}),"\n",(0,s.jsx)(n.li,{children:"Social zone (120-360 cm): Formal interactions"}),"\n",(0,s.jsx)(n.li,{children:"Public zone (360+ cm): Public speaking distance"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"interaction-quality-metric",children:"Interaction Quality Metric"}),"\n",(0,s.jsx)(n.p,{children:"A metric for evaluating HRI effectiveness:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"IQ = \u03b1*Comprehension + \u03b2*Efficiency + \u03b3*Safety + \u03b4*Acceptability\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"\u03b1"}),", ",(0,s.jsx)(n.code,{children:"\u03b2"}),", ",(0,s.jsx)(n.code,{children:"\u03b3"}),", ",(0,s.jsx)(n.code,{children:"\u03b4"})," are weights that sum to 1"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Comprehension"}),": How well the human understands the robot"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Efficiency"}),": How quickly and effectively tasks are completed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Safety"}),": How safely the interaction proceeds"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Acceptability"}),": How comfortable the human feels"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"code-example-human-robot-interaction-system",children:"Code Example: Human-Robot Interaction System"}),"\n",(0,s.jsx)(n.p,{children:"Here's an implementation of an HRI system for a humanoid robot:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport time\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any, Optional, Callable\nimport threading\nimport queue\nfrom enum import Enum\n\n\nclass SocialContext(Enum):\n    """Different social contexts for HRI"""\n    COLLABORATIVE = "collaborative"\n    INSTRUCTIVE = "instructive"\n    COMPANION = "companion"\n    UTILITY = "utility"\n\n\nclass InteractionModality(Enum):\n    """Different interaction modalities"""\n    SPEECH = "speech"\n    GESTURE = "gesture"\n    FACE = "face"\n    TOUCH = "touch"  # If robot has tactile sensors\n    PROXEMICS = "proxemics"\n\n\n@dataclass\nclass HumanState:\n    """Represents the state of a human in interaction"""\n    position: List[float] = None  # x, y, z coordinates\n    orientation: List[float] = None  # quaternion [w, x, y, z]\n    facial_expression: Optional[str] = None\n    gestures: List[str] = None\n    attention_focus: Optional[str] = None  # What the human is looking at\n    engagement_level: float = 0.5  # 0-1 scale\n    comfort_level: float = 0.7  # 0-1 scale\n    cultural_background: Optional[str] = None\n    timestamp: float = 0.0\n\n\n@dataclass\nclass RobotState:\n    """Represents the state of the robot in interaction"""\n    position: List[float] = None  # x, y, z coordinates\n    orientation: List[float] = None  # quaternion [w, x, y, z]\n    head_orientation: List[float] = None  # Where robot head is facing\n    gesture: Optional[str] = None  # Current expressive gesture\n    facial_display: Optional[str] = None  # Facial expression\n    speech_output: Optional[str] = None\n    proximity_distance: float = 1.0  # Current distance to human\n    social_stance: str = "neutral"  # friendly, formal, etc.\n    social_context: SocialContext = SocialContext.COLLABORATIVE\n    comfort_zone: float = 1.0  # Preferred distance for current interaction\n\n\nclass GestureRecognizer:\n    """Component to recognize human gestures"""\n    \n    def __init__(self):\n        self.known_gestures = {\n            "wave": ["arm_raised", "hand_open", "waving_motion"],\n            "point": ["arm_extended", "finger_extended", "direction_indicated"],\n            "stop": ["arm_extended", "palm_facing_forward"],\n            "come_here": ["arm_extended", "palm_facing_down", "beckoning_motion"],\n            "thumbs_up": ["thumb_extended", "other_fingers_folded"],\n        }\n        self.gesture_threshold = 0.7  # Confidence threshold for gesture recognition\n    \n    def recognize_gesture(self, human_state: HumanState) -> Optional[str]:\n        """Recognize gesture from human state data"""\n        # In real implementation, this would analyze pose data\n        # For this example, we\'ll simulate gesture recognition\n        \n        # Check if human is making a known gesture pattern\n        if human_state.gestures:\n            for gesture in human_state.gestures:\n                if gesture in self.known_gestures:\n                    return gesture\n        \n        return None\n\n\nclass SpeechInterpreter:\n    """Component to interpret natural language commands"""\n    \n    def __init__(self):\n        self.intent_keywords = {\n            "greeting": ["hello", "hi", "hey", "good morning", "good afternoon"],\n            "farewell": ["bye", "goodbye", "see you", "good night"],\n            "command": ["please", "could you", "can you", "move", "go", "come", "help"],\n            "navigation": ["to the", "toward", "towards", "at", "by", "near"],\n            "manipulation": ["pick up", "grasp", "take", "give", "hand me", "place"],\n            "confirmation": ["yes", "ok", "okay", "sure", "affirmative"],\n            "negation": ["no", "stop", "cancel", "negative", "incorrect"]\n        }\n    \n    def interpret_speech(self, text: str) -> Dict[str, Any]:\n        """Interpret natural language input"""\n        text_lower = text.lower()\n        result = {\n            "intents": [],\n            "entities": [],\n            "confidence": 0.0,\n            "parsed_command": None\n        }\n        \n        # Identify intents\n        for intent, keywords in self.intent_keywords.items():\n            for keyword in keywords:\n                if keyword in text_lower:\n                    result["intents"].append(intent)\n                    break\n        \n        # Extract entities (simplified)\n        words = text_lower.split()\n        for i, word in enumerate(words):\n            # Look for spatial references\n            if word in ["kitchen", "bedroom", "table", "room", "area"]:\n                result["entities"].append({"type": "location", "value": word})\n            elif word in ["cup", "bottle", "box", "object"]:\n                result["entities"].append({"type": "object", "value": word})\n            elif word in ["left", "right", "forward", "backward", "up", "down"]:\n                result["entities"].append({"type": "direction", "value": word})\n        \n        # Calculate confidence based on intent recognition\n        result["confidence"] = min(1.0, len(result["intents"]) * 0.3)\n        \n        # Generate parsed command if possible\n        if "navigation" in result["intents"] and any(e["type"] == "location" for e in result["entities"]):\n            location = next(e["value"] for e in result["entities"] if e["type"] == "location")\n            result["parsed_command"] = {"action": "navigate", "target": location}\n        elif "manipulation" in result["intents"] and any(e["type"] == "object" for e in result["entities"]):\n            obj = next(e["value"] for e in result["entities"] if e["type"] == "object")\n            result["parsed_command"] = {"action": "grasp", "target": obj}\n        \n        return result\n\n\nclass RobotExpressiveness:\n    """Component to manage robot expressiveness and social behaviors"""\n    \n    def __init__(self):\n        self.expression_mappings = {\n            "happy": ["smile", "head_nod", "eyebrow_raise"],\n            "attentive": ["direct_gaze", "head_tilt", "active_posture"],\n            "confused": ["head_shake", "eyebrow_furrow", "pause_gesture"],\n            "apologetic": ["head_bow", "palm_out", "submissive_posture"],\n            "greeting": ["wave", "smile", "direct_gaze"]\n        }\n        \n        # Emotional state machine\n        self.current_emotion = "neutral"\n        self.emotion_intensity = 0.5\n    \n    def generate_expressive_behavior(self, emotion: str, intensity: float = 0.5) -> Dict[str, Any]:\n        """Generate appropriate expressive behavior for emotion"""\n        self.current_emotion = emotion\n        self.emotion_intensity = intensity\n        \n        if emotion in self.expression_mappings:\n            expressions = self.expression_mappings[emotion]\n            return {\n                "facial": expressions[0] if len(expressions) > 0 else "neutral",\n                "head_movement": expressions[1] if len(expressions) > 1 else "neutral",\n                "body_posture": expressions[2] if len(expressions) > 2 else "neutral",\n                "gesture": expressions[0] if len(expressions) > 0 else "",\n                "intensity": intensity\n            }\n        else:\n            return {\n                "facial": "neutral",\n                "head_movement": "neutral",\n                "body_posture": "neutral",\n                "gesture": "",\n                "intensity": 0.0\n            }\n    \n    def adjust_proxemics(self, context: SocialContext, comfort_level: float) -> float:\n        """Adjust preferred distance based on social context and comfort"""\n        base_distances = {\n            SocialContext.COLLABORATIVE: 0.8,  # Close for collaboration\n            SocialContext.INSTRUCTIVE: 1.2,   # Moderate for instruction\n            SocialContext.COMPANION: 1.0,     # Comfortable for companionship\n            SocialContext.UTILITY: 1.5        # More distance for utility tasks\n        }\n        \n        base_distance = base_distances.get(context, 1.0)\n        \n        # Adjust based on comfort level (higher comfort = less distance)\n        adjusted_distance = base_distance * (1.5 - comfort_level)\n        \n        return max(0.5, min(2.0, adjusted_distance))  # Keep within reasonable bounds\n\n\nclass SocialInteractionManager:\n    """Main manager for social interactions"""\n    \n    def __init__(self):\n        self.gesture_recognizer = GestureRecognizer()\n        self.speech_interpreter = SpeechInterpreter()\n        self.expressiveness = RobotExpressiveness()\n        \n        self.human_state = HumanState()\n        self.robot_state = RobotState()\n        \n        self.interaction_history = []\n        self.social_rules = {\n            "maintain_eye_contact": True,\n            "respect_personal_space": True,\n            "acknowledge_gestures": True,\n            "use_appropriate_tone": True\n        }\n        \n        self.response_generation_enabled = True\n        self.response_queue = queue.Queue()\n    \n    def update_human_state(self, new_state: HumanState):\n        """Update the recognized state of the human"""\n        # Merge new state with current state\n        if new_state.position:\n            self.human_state.position = new_state.position\n        if new_state.orientation:\n            self.human_state.orientation = new_state.orientation\n        if new_state.facial_expression:\n            self.human_state.facial_expression = new_state.facial_expression\n        if new_state.gestures is not None:\n            self.human_state.gestures = new_state.gestures\n        if new_state.attention_focus:\n            self.human_state.attention_focus = new_state.attention_focus\n        if new_state.engagement_level:\n            self.human_state.engagement_level = new_state.engagement_level\n        if new_state.comfort_level:\n            self.human_state.comfort_level = new_state.comfort_level\n        if new_state.cultural_background:\n            self.human_state.cultural_background = new_state.cultural_background\n        \n        self.human_state.timestamp = new_state.timestamp or time.time()\n    \n    def update_robot_state(self, new_state: RobotState):\n        """Update the robot\'s current state"""\n        if new_state.position:\n            self.robot_state.position = new_state.position\n        if new_state.orientation:\n            self.robot_state.orientation = new_state.orientation\n        if new_state.head_orientation:\n            self.robot_state.head_orientation = new_state.head_orientation\n        if new_state.gesture:\n            self.robot_state.gesture = new_state.gesture\n        if new_state.facial_display:\n            self.robot_state.facial_display = new_state.facial_display\n        if new_state.speech_output:\n            self.robot_state.speech_output = new_state.speech_output\n        if new_state.proximity_distance:\n            self.robot_state.proximity_distance = new_state.proximity_distance\n        if new_state.social_stance:\n            self.robot_state.social_stance = new_state.social_stance\n        if new_state.social_context:\n            self.robot_state.social_context = new_state.social_context\n        if new_state.comfort_zone:\n            self.robot_state.comfort_zone = new_state.comfort_zone\n    \n    def process_interaction(self, input_type: InteractionModality, input_data: Any) -> Dict[str, Any]:\n        """Process incoming interaction input"""\n        result = {\n            "processed_as": input_type.value,\n            "interpretation": None,\n            "response": None,\n            "expressive_behavior": None,\n            "timestamp": time.time()\n        }\n        \n        if input_type == InteractionModality.SPEECH:\n            interpretation = self.speech_interpreter.interpret_speech(input_data)\n            result["interpretation"] = interpretation\n            response = self._generate_speech_response(interpretation)\n            result["response"] = response\n            \n        elif input_type == InteractionModality.GESTURE:\n            # Gesture recognition happens implicitly through human state\n            if self.human_state.gestures:\n                recognized = [g for g in self.human_state.gestures if self.gesture_recognizer.recognize_gesture(self.human_state)]\n                result["interpretation"] = {"recognized_gestures": recognized}\n                response = self._generate_gesture_response(recognized)\n                result["response"] = response\n        \n        elif input_type == InteractionModality.FACE:\n            # Handle facial expressions\n            if self.human_state.facial_expression:\n                response = self._respond_to_facial_expression(self.human_state.facial_expression)\n                result["response"] = response\n        \n        # Generate appropriate expressive behavior based on interaction\n        expressiveness = self.expressiveness.generate_expressive_behavior(\n            self._determine_robot_emotion(),\n            self.human_state.engagement_level\n        )\n        result["expressive_behavior"] = expressiveness\n        \n        # Update interaction history\n        self.interaction_history.append(result)\n        \n        return result\n    \n    def _generate_speech_response(self, interpretation: Dict[str, Any]) -> Optional[str]:\n        """Generate appropriate speech response to interpreted input"""\n        if not interpretation.get("intents"):\n            return "I\'m sorry, I didn\'t understand. Could you repeat that?"\n        \n        intents = interpretation["intents"]\n        \n        if "greeting" in intents:\n            return "Hello! How can I assist you today?"\n        elif "farewell" in intents:\n            return "Goodbye! Have a great day!"\n        elif "command" in intents:\n            if interpretation.get("parsed_command"):\n                cmd = interpretation["parsed_command"]\n                if cmd["action"] == "navigate":\n                    return f"OK, going to the {cmd[\'target\']}."\n                elif cmd["action"] == "grasp":\n                    return f"OK, picking up the {cmd[\'target\']}."\n            return "I understand you want me to do something. Could you be more specific?"\n        elif "confirmation" in intents:\n            return "Great! I\'m glad that\'s correct."\n        elif "negation" in intents:\n            return "I apologize. How can I help differently?"\n        else:\n            return "I understand. How can I assist you further?"\n    \n    def _generate_gesture_response(self, recognized_gestures: List[str]) -> Optional[str]:\n        """Generate appropriate response to recognized gestures"""\n        if not recognized_gestures:\n            return None\n        \n        for gesture in recognized_gestures:\n            if gesture == "wave":\n                return "Hello! Nice to meet you!"\n            elif gesture == "point":\n                # In real implementation, this would look in the pointed direction\n                return "I see what you\'re pointing at."\n            elif gesture == "stop":\n                return "I will stop. Is everything alright?"\n            elif gesture == "come_here":\n                return "I\'m coming over now."\n            elif gesture == "thumbs_up":\n                return "Thank you for the positive feedback!"\n        \n        return "I noticed your gesture."\n    \n    def _respond_to_facial_expression(self, expression: str) -> Optional[str]:\n        """Generate appropriate response to human facial expression"""\n        if expression == "smile":\n            return "You seem happy! I\'m glad to see that."\n        elif expression == "frown":\n            return "Is something wrong? How can I help?"\n        elif expression == "surprised":\n            return "Did I surprise you? Sorry if I did!"\n        else:\n            return "I notice your expression."\n    \n    def _determine_robot_emotion(self) -> str:\n        """Determine appropriate robot emotional response"""\n        # Determine emotion based on human state\n        if self.human_state.engagement_level > 0.7:\n            if self.human_state.facial_expression == "smile":\n                return "happy"\n            elif self.human_state.facial_expression == "frown":\n                return "concerned"\n            else:\n                return "attentive"\n        elif self.human_state.engagement_level < 0.3:\n            return "patient"\n        else:\n            return "neutral"\n    \n    def get_social_recommendations(self) -> List[str]:\n        """Get recommendations for social behavior based on current state"""\n        recommendations = []\n        \n        # Check proxemics\n        current_distance = np.linalg.norm(\n            np.array(self.human_state.position or [0, 0, 0]) - \n            np.array(self.robot_state.position or [0, 0, 0])\n        ) if self.human_state.position and self.robot_state.position else 1.0\n        \n        preferred_distance = self.expressiveness.adjust_proxemics(\n            self.robot_state.social_context,\n            self.human_state.comfort_level\n        )\n        \n        if current_distance > preferred_distance * 1.5:\n            recommendations.append("Consider moving closer to the human")\n        elif current_distance < preferred_distance * 0.7:\n            recommendations.append("Consider giving more personal space")\n        \n        # Check engagement\n        if self.human_state.engagement_level < 0.3:\n            recommendations.append("Try to engage the human more actively")\n        \n        # Check comfort\n        if self.human_state.comfort_level < 0.4:\n            recommendations.append("Slow down interactions to improve comfort")\n        \n        return recommendations\n    \n    def set_social_context(self, context: SocialContext):\n        """Set the social context for current interaction"""\n        self.robot_state.social_context = context\n        # Adjust behavior based on new context\n        new_distance = self.expressiveness.adjust_proxemics(context, self.human_state.comfort_level)\n        self.robot_state.comfort_zone = new_distance\n\n\nclass HRIController:\n    """Main controller for Human-Robot Interaction systems"""\n    \n    def __init__(self):\n        self.social_manager = SocialInteractionManager()\n        self.is_active = False\n        self.interaction_thread = None\n        \n        # Initialize robot in neutral state\n        initial_robot_state = RobotState(\n            position=[0.0, 0.0, 0.0],\n            orientation=[1.0, 0.0, 0.0, 0.0],\n            head_orientation=[0.0, 0.0, 0.0, 1.0],\n            social_stance="friendly",\n            social_context=SocialContext.COLLABORATIVE,\n            comfort_zone=1.0\n        )\n        self.social_manager.update_robot_state(initial_robot_state)\n    \n    def start_interaction_system(self):\n        """Start the HRI system"""\n        self.is_active = True\n        print("Human-Robot Interaction system activated")\n        \n        # Start interaction thread\n        self.interaction_thread = threading.Thread(target=self._interaction_worker)\n        self.interaction_thread.start()\n    \n    def stop_interaction_system(self):\n        """Stop the HRI system"""\n        self.is_active = False\n        if self.interaction_thread:\n            self.interaction_thread.join()\n        print("Human-Robot Interaction system deactivated")\n    \n    def _interaction_worker(self):\n        """Background worker for continuous interaction monitoring"""\n        while self.is_active:\n            try:\n                # Check for any changes in human state that should trigger responses\n                # In real implementation, this would process sensor data continuously\n                time.sleep(0.1)  # Small sleep to prevent busy waiting\n            except Exception as e:\n                print(f"HRI worker error: {e}")\n                time.sleep(1)  # Longer sleep on error\n    \n    def process_user_input(self, modality: InteractionModality, data: Any) -> Dict[str, Any]:\n        """Process user input through the HRI system"""\n        result = self.social_manager.process_interaction(modality, data)\n        \n        # Update robot state based on interaction\n        if result.get("expressive_behavior"):\n            expr = result["expressive_behavior"]\n            new_robot_state = RobotState(\n                facial_display=expr["facial"],\n                gesture=expr["gesture"],\n                social_stance=self.social_manager.robot_state.social_stance,\n                social_context=self.social_manager.robot_state.social_context\n            )\n            self.social_manager.update_robot_state(new_robot_state)\n        \n        return result\n    \n    def set_human_state(self, human_state: HumanState):\n        """Set the current state of the human"""\n        self.social_manager.update_human_state(human_state)\n    \n    def get_interaction_status(self) -> Dict[str, Any]:\n        """Get current status of the HRI system"""\n        return {\n            "is_active": self.is_active,\n            "human_state": {\n                "engagement": self.social_manager.human_state.engagement_level,\n                "comfort": self.social_manager.human_state.comfort_level,\n                "position": self.social_manager.human_state.position,\n                "expression": self.social_manager.human_state.facial_expression\n            },\n            "robot_state": {\n                "social_context": self.social_manager.robot_state.social_context.value,\n                "proximity_distance": self.social_manager.robot_state.proximity_distance,\n                "comfort_zone": self.social_manager.robot_state.comfort_zone\n            },\n            "recommendations": self.social_manager.get_social_recommendations(),\n            "interaction_count": len(self.social_manager.interaction_history)\n        }\n\n\ndef main():\n    """Example usage of the Human-Robot Interaction system"""\n    print("Human-Robot Interaction System Example")\n    \n    # Initialize the HRI system\n    hri_controller = HRIController()\n    hri_controller.start_interaction_system()\n    \n    # Simulate a human approaching the robot\n    human_approach = HumanState(\n        position=[1.0, 0.0, 0.0],\n        facial_expression="neutral",\n        gestures=["wave"],\n        engagement_level=0.6,\n        comfort_level=0.7,\n        timestamp=time.time()\n    )\n    \n    hri_controller.set_human_state(human_approach)\n    \n    print("\\n--- Human Approaches Robot ---")\n    print(f"Human position: {human_approach.position}")\n    print(f"Engagement level: {human_approach.engagement_level}")\n    print(f"Comfort level: {human_approach.comfort_level}")\n    \n    # Process the greeting gesture\n    gesture_result = hri_controller.process_user_input(InteractionModality.GESTURE, human_approach.gestures)\n    print(f"Gesture response: {gesture_result[\'response\']}")\n    print(f"Robot expression: {gesture_result[\'expressive_behavior\'][\'facial\']}")\n    \n    # Simulate speech input\n    speech_input = "Hello, could you please go to the kitchen?"\n    print(f"\\n--- Speech Input: \'{speech_input}\' ---")\n    \n    speech_result = hri_controller.process_user_input(InteractionModality.SPEECH, speech_input)\n    print(f"Interpretation: {speech_result[\'interpretation\']}")\n    print(f"Robot response: {speech_result[\'response\']}")\n    \n    # Check system status\n    status = hri_controller.get_interaction_status()\n    print(f"\\n--- System Status ---")\n    print(f"Active: {status[\'is_active\']}")\n    print(f"Human engagement: {status[\'human_state\'][\'engagement\']}")\n    print(f"Human comfort: {status[\'human_state\'][\'comfort\']}")\n    print(f"Recommendations: {status[\'recommendations\']}")\n    print(f"Interactions processed: {status[\'interaction_count\']}")\n    \n    # Change social context\n    hri_controller.social_manager.set_social_context(SocialContext.INSTRUCTIVE)\n    print(f"\\n--- Changed context to: {hri_controller.social_manager.robot_state.social_context.value} ---")\n    \n    # Simulate another interaction\n    follow_up = "Can you pick up the red cup?"\n    print(f"\\n--- Follow-up: \'{follow_up}\' ---")\n    \n    follow_result = hri_controller.process_user_input(InteractionModality.SPEECH, follow_up)\n    print(f"Response: {follow_result[\'response\']}")\n    print(f"Interpretation: {follow_result[\'interpretation\']}")\n    \n    # Final status\n    final_status = hri_controller.get_interaction_status()\n    print(f"\\n--- Final Status ---")\n    print(f"Social context: {final_status[\'robot_state\'][\'social_context\']}")\n    print(f"Comfort zone: {final_status[\'robot_state\'][\'comfort_zone\']}")\n    \n    # Stop the system\n    hri_controller.stop_interaction_system()\n    \n    print("\\nHuman-Robot Interaction system example completed")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"simulation-demonstration",children:"Simulation Demonstration"}),"\n",(0,s.jsx)(n.p,{children:"This implementation demonstrates a comprehensive Human-Robot Interaction system with support for multiple interaction modalities, social behaviors, and contextual awareness. The system can interpret natural language commands, recognize gestures, maintain appropriate social distances, and respond with appropriate robotic expressions. The code can be integrated with ROS 2 and humanoid robot simulation environments to create natural and engaging human-robot interactions."}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-lab-human-robot-interaction-implementation",children:"Hands-On Lab: Human-Robot Interaction Implementation"}),"\n",(0,s.jsx)(n.p,{children:"In this lab, you'll implement and test a complete HRI system:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up the social interaction manager component"}),"\n",(0,s.jsx)(n.li,{children:"Implement multimodal interaction processing"}),"\n",(0,s.jsx)(n.li,{children:"Create expressive behaviors for the robot"}),"\n",(0,s.jsx)(n.li,{children:"Test with various human inputs (speech, gestures, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the effectiveness of the HRI system"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"required-equipment",children:"Required Equipment:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ROS 2 Humble environment"}),"\n",(0,s.jsx)(n.li,{children:"Robot simulation environment (Gazebo, Isaac Sim)"}),"\n",(0,s.jsx)(n.li,{children:"(Optional) Physical humanoid robot"}),"\n",(0,s.jsx)(n.li,{children:"Speech recognition library"}),"\n",(0,s.jsx)(n.li,{children:"Computer vision libraries for gesture recognition"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"instructions",children:"Instructions:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create a new ROS 2 package: ",(0,s.jsx)(n.code,{children:"ros2 pkg create --build-type ament_python hri_system"})]}),"\n",(0,s.jsx)(n.li,{children:"Implement the SocialInteractionManager and HRIController classes"}),"\n",(0,s.jsx)(n.li,{children:"Create launch files to start the HRI node"}),"\n",(0,s.jsx)(n.li,{children:"Test with different social contexts (collaborative, companion, etc.)"}),"\n",(0,s.jsx)(n.li,{children:"Implement gesture recognition and speech interpretation"}),"\n",(0,s.jsx)(n.li,{children:"Test with real or simulated human inputs"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the quality of interaction using metrics"}),"\n",(0,s.jsx)(n.li,{children:"Document the effectiveness of different interaction modalities"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"common-pitfalls--debugging-notes",children:"Common Pitfalls & Debugging Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cultural Sensitivity"}),": Different cultures have varying norms for personal space and interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Privacy Concerns"}),": Facial recognition and tracking raise privacy issues"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Misinterpretation"}),": Misunderstanding human intent can lead to inappropriate responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Timing Issues"}),": Delays in response can make interactions feel unnatural"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Over-Animation"}),": Too many expressions can be distracting or creepy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Technical Limitations"}),": Current technology may not support all desired capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety Considerations"}),": Close interactions require appropriate safety measures"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary--key-terms",children:"Summary & Key Terms"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Terms:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Human-Robot Interaction (HRI)"}),": Study of interactions between humans and robots"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social Robotics"}),": Field focusing on robots that interact socially with humans"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multimodal Interaction"}),": Using multiple input/output modalities for interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Proxemics"}),": Study of personal space and spatial relationships in interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Theory of Mind"}),": Understanding others' mental states and beliefs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Joint Attention"}),": Focusing on the same object or activity together"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Social Presence"}),": Feeling that the robot is a social entity"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading--citifications",children:"Further Reading & Citifications"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'Breazeal, C. (2003). "Toward sociable robots." Robotics and Autonomous Systems.'}),"\n",(0,s.jsx)(n.li,{children:'Fong, T., et al. (2003). "A survey of socially interactive robots." Robotics and Autonomous Systems.'}),"\n",(0,s.jsx)(n.li,{children:'Kidd, C. D., & Breazeal, C. (2008). "Robots at home: Understanding long-term human-robot interaction." IEEE International Conference on Robotics and Automation.'}),"\n",(0,s.jsx)(n.li,{children:'Mataric, M. J., et al. (2007). "Socially assistive robotics." IEEE Intelligent Systems.'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Explain the key principles of social robotics for humanoid robots."}),"\n",(0,s.jsx)(n.li,{children:"What are the main modalities used in human-robot interaction?"}),"\n",(0,s.jsx)(n.li,{children:"How does proxemics apply to human-robot interaction design?"}),"\n",(0,s.jsx)(n.li,{children:"Describe the Theory of Mind model and its application in HRI."}),"\n",(0,s.jsx)(n.li,{children:"What safety and ethical considerations are important in HRI systems?"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous"}),": ",(0,s.jsx)(n.a,{href:"../locomotion.md",children:"Bipedal Locomotion and Walking Control"}),(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"../../07-capstone/intro.md",children:"Capstone Project Introduction"})]})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var i=t(6540);const s={},o=i.createContext(s);function a(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);