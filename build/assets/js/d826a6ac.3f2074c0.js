"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[902],{7888:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"vla-robotics/whisper","title":"Whisper for Voice Recognition","description":"Whisper is a state-of-the-art automatic speech recognition (ASR) system developed by OpenAI that plays a crucial role in Vision-Language-Action (VLA) robotic systems. It enables robots to understand natural language commands through speech, providing an intuitive and human-like interaction modality. Whisper\'s robust performance across multiple languages and its ability to handle varying audio conditions make it valuable for robotics applications.","source":"@site/docs/06-vla-robotics/whisper.md","sourceDirName":"06-vla-robotics","slug":"/vla-robotics/whisper","permalink":"/Hackthon_SpecKitPlus/docs/vla-robotics/whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/shaheryarshah/Hackthon_SpecKitPlus/edit/main/docs/docs/06-vla-robotics/whisper.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action Robotics","permalink":"/Hackthon_SpecKitPlus/docs/vla-robotics/intro"},"next":{"title":"LLM-Based Planning for Robotics","permalink":"/Hackthon_SpecKitPlus/docs/vla-robotics/llm-planning"}}');var s=i(4848),o=i(8453);const r={},a="Whisper for Voice Recognition",c={},l=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Transformer Architecture",id:"transformer-architecture",level:3},{value:"Audio Preprocessing",id:"audio-preprocessing",level:3},{value:"Confidence Scoring",id:"confidence-scoring",level:3},{value:"Real-time vs Batch Processing",id:"real-time-vs-batch-processing",level:3},{value:"Equations and Models",id:"equations-and-models",level:2},{value:"Audio Feature Extraction",id:"audio-feature-extraction",level:3},{value:"Transformer Attention Mechanism",id:"transformer-attention-mechanism",level:3},{value:"Confidence in Transcription",id:"confidence-in-transcription",level:3},{value:"Code Example: Whisper Integration for Robotics",id:"code-example-whisper-integration-for-robotics",level:2},{value:"Simulation Demonstration",id:"simulation-demonstration",level:2},{value:"Hands-On Lab: Whisper Integration for Robotics",id:"hands-on-lab-whisper-integration-for-robotics",level:2},{value:"Required Equipment:",id:"required-equipment",level:3},{value:"Instructions:",id:"instructions",level:3},{value:"Common Pitfalls &amp; Debugging Notes",id:"common-pitfalls--debugging-notes",level:2},{value:"Summary &amp; Key Terms",id:"summary--key-terms",level:2},{value:"Further Reading &amp; Citations",id:"further-reading--citations",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"whisper-for-voice-recognition",children:"Whisper for Voice Recognition"})}),"\n",(0,s.jsx)(n.p,{children:"Whisper is a state-of-the-art automatic speech recognition (ASR) system developed by OpenAI that plays a crucial role in Vision-Language-Action (VLA) robotic systems. It enables robots to understand natural language commands through speech, providing an intuitive and human-like interaction modality. Whisper's robust performance across multiple languages and its ability to handle varying audio conditions make it valuable for robotics applications."}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"After completing this section, you should be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the architecture and capabilities of the Whisper ASR system"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Whisper into robotic systems for voice command processing"}),"\n",(0,s.jsx)(n.li,{children:"Preprocess audio inputs for optimal Whisper performance"}),"\n",(0,s.jsx)(n.li,{children:"Handle Whisper outputs and integrate them with downstream NLP systems"}),"\n",(0,s.jsx)(n.li,{children:"Address common challenges in deploying Whisper in robotics environments"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the accuracy and latency of speech recognition in robotic contexts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,s.jsx)(n.h3,{id:"transformer-architecture",children:"Transformer Architecture"}),"\n",(0,s.jsx)(n.p,{children:"Whisper is built on a transformer architecture that consists of:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Encoder"}),": Processes audio inputs with convolutional feature extraction followed by transformer layers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decoder"}),": Generates text outputs using cross-attention to the encoded audio features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multilingual Capability"}),": Trained on 98+ languages, enabling recognition across diverse linguistic contexts"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"audio-preprocessing",children:"Audio Preprocessing"}),"\n",(0,s.jsx)(n.p,{children:"For effective Whisper performance, audio preprocessing includes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Resampling"}),": Converting audio to the required sample rate (typically 16kHz)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Normalization"}),": Adjusting audio levels for consistent processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Noise Reduction"}),": Filtering out background noise when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Segmentation"}),": Breaking long audio streams into appropriate chunks"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"confidence-scoring",children:"Confidence Scoring"}),"\n",(0,s.jsx)(n.p,{children:"Whisper provides confidence scores for transcriptions, which is important for robotics applications where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Low-confidence transcriptions may require confirmation"}),"\n",(0,s.jsx)(n.li,{children:"High-confidence transcriptions can be processed immediately"}),"\n",(0,s.jsx)(n.li,{children:"Confidence levels can trigger different system behaviors"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"real-time-vs-batch-processing",children:"Real-time vs Batch Processing"}),"\n",(0,s.jsx)(n.p,{children:"Whisper can be used in:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Batch Mode"}),": For processing complete audio clips with high accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Streaming Mode"}),": For real-time processing (with additional engineering considerations)"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"equations-and-models",children:"Equations and Models"}),"\n",(0,s.jsx)(n.h3,{id:"audio-feature-extraction",children:"Audio Feature Extraction"}),"\n",(0,s.jsx)(n.p,{children:"Whisper processes audio by transforming it into features:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"F = STFT(x, window, n_fft)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"F"})," is the spectrogram of the audio signal"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"STFT"})," is the Short-Time Fourier Transform"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"x"})," is the input audio signal"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"window"})," is the window function (typically Hann window)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"n_fft"})," is the FFT size"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"transformer-attention-mechanism",children:"Transformer Attention Mechanism"}),"\n",(0,s.jsx)(n.p,{children:"The attention mechanism in Whisper's transformer layers:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Attention(Q, K, V) = softmax(QK^T / \u221ad_k)V\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Q"})," is the query matrix (input representation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"K"})," is the key matrix (input representation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"V"})," is the value matrix (input representation)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"d_k"})," is the dimension of the key vectors"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"confidence-in-transcription",children:"Confidence in Transcription"}),"\n",(0,s.jsx)(n.p,{children:"The confidence of a generated sequence can be approximated by:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Confidence(S) = \u03a0_i P(w_i | w_1, ..., w_{i-1}, F)\n"})}),"\n",(0,s.jsx)(n.p,{children:"Where:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"S"})," is the transcript sequence ",(0,s.jsx)(n.code,{children:"w_1, w_2, ..., w_n"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"F"})," is the audio features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"P(w_i | ...)"})," is the probability of word ",(0,s.jsx)(n.code,{children:"w_i"})," given context"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"code-example-whisper-integration-for-robotics",children:"Code Example: Whisper Integration for Robotics"}),"\n",(0,s.jsx)(n.p,{children:"Here's an example of integrating Whisper for voice recognition in a robotics system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import asyncio\nimport numpy as np\nimport torch\nimport whisper\nimport speech_recognition as sr\nimport queue\nimport threading\nimport time\nfrom dataclasses import dataclass\nfrom typing import Optional, Dict, Any\n\n\n@dataclass\nclass VoiceCommand:\n    \"\"\"Represents a recognized voice command with metadata\"\"\"\n    text: str\n    confidence: float\n    timestamp: float\n    language: str\n    audio_duration: float\n\n\nclass WhisperRobotListener:\n    \"\"\"\n    Whisper-based voice command recognizer for robotics\n    \"\"\"\n    def __init__(self, model_size=\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        \"\"\"\n        Initialize the Whisper-based voice recognition system\n        \n        :param model_size: Size of Whisper model ('tiny', 'base', 'small', 'medium', 'large')\n        :param device: Device to run the model on ('cpu', 'cuda', 'mps')\n        \"\"\"\n        self.device = device\n        self.model = whisper.load_model(model_size, device=device)\n        \n        # Initialize the speech recognizer for audio capture\n        self.speech_recognizer = sr.Recognizer()\n        self.speech_recognizer.energy_threshold = 300  # Adjust based on environment\n        self.speech_recognizer.dynamic_energy_threshold = True\n        \n        # Audio queue for streaming\n        self.audio_queue = queue.Queue()\n        self.is_listening = False\n        self.listening_thread = None\n        \n        # Statistics\n        self.stats = {\n            'total_commands': 0,\n            'avg_processing_time': 0.0,\n            'avg_confidence': 0.0,\n            'language_distribution': {}\n        }\n        \n        print(f\"Whisper model loaded on {device}\")\n    \n    def transcribe_audio_file(self, audio_file_path):\n        \"\"\"\n        Transcribe audio from a file using Whisper\n        \n        :param audio_file_path: Path to audio file\n        :return: VoiceCommand with transcription and metadata\n        \"\"\"\n        start_time = time.time()\n        \n        # Load and transcribe audio\n        result = self.model.transcribe(audio_file_path)\n        \n        processing_time = time.time() - start_time\n        \n        # Extract transcription details\n        text = result['text'].strip()\n        confidence = self._estimate_confidence(result)\n        language = result.get('language', 'unknown')\n        audio_duration = result.get('duration', 0.0)\n        \n        command = VoiceCommand(\n            text=text,\n            confidence=confidence,\n            timestamp=time.time(),\n            language=language,\n            audio_duration=audio_duration\n        )\n        \n        # Update statistics\n        self._update_statistics(command, processing_time)\n        \n        return command\n    \n    def transcribe_audio_buffer(self, audio_buffer):\n        \"\"\"\n        Transcribe raw audio buffer using Whisper\n        \n        :param audio_buffer: Audio data in bytes\n        :return: VoiceCommand with transcription and metadata\n        \"\"\"\n        # Convert audio buffer to audio data\n        audio_data = sr.AudioData(audio_buffer, 16000, 2)  # Assuming 16kHz, 16-bit\n        \n        # Save to temporary file (Whisper expects file input)\n        import io\n        import wave\n        temp_filename = f\"temp_audio_{int(time.time())}.wav\"\n        \n        # Write raw audio data to WAV file\n        with wave.open(temp_filename, 'wb') as wav_file:\n            wav_file.setnchannels(1)  # Mono\n            wav_file.setsampwidth(2)  # 16-bit\n            wav_file.setframerate(16000)  # 16kHz\n            wav_file.writeframes(audio_buffer)\n        \n        try:\n            # Transcribe the temporary file\n            result = self.model.transcribe(temp_filename)\n            \n            # Clean up\n            import os\n            os.remove(temp_filename)\n            \n            # Extract transcription details\n            text = result['text'].strip()\n            confidence = self._estimate_confidence(result)\n            language = result.get('language', 'unknown')\n            audio_duration = result.get('duration', len(audio_buffer) / (16000 * 2))  # Estimate\n            \n            command = VoiceCommand(\n                text=text,\n                confidence=confidence,\n                timestamp=time.time(),\n                language=language,\n                audio_duration=audio_duration\n            )\n            \n            return command\n        except Exception as e:\n            print(f\"Error transcribing audio buffer: {e}\")\n            import os\n            if os.path.exists(temp_filename):\n                os.remove(temp_filename)\n            return None\n    \n    def _estimate_confidence(self, result):\n        \"\"\"\n        Estimate confidence from Whisper result\n        Note: Whisper doesn't provide direct confidence scores, so we'll use a heuristic\n        \"\"\"\n        # In real implementations, you might use token probabilities or other metrics\n        # For now, using a simple heuristic based on the most common tokens\n        if 'segments' in result and len(result['segments']) > 0:\n            # Average the temperatures across segments as a proxy for confidence\n            temp_sum = sum([seg.get('temperature', 0.0) for seg in result['segments']])\n            avg_temp = temp_sum / len(result['segments'])\n            # Convert temperature to confidence (lower temperature = higher confidence)\n            confidence = max(0.0, 1.0 - avg_temp)\n        else:\n            confidence = 0.8  # Default confidence\n        \n        return confidence\n    \n    def _update_statistics(self, command, processing_time):\n        \"\"\"Update internal statistics\"\"\"\n        self.stats['total_commands'] += 1\n        old_avg = self.stats['avg_processing_time']\n        self.stats['avg_processing_time'] = (\n            (old_avg * (self.stats['total_commands'] - 1) + processing_time) / \n            self.stats['total_commands']\n        )\n        \n        old_conf_avg = self.stats['avg_confidence']\n        self.stats['avg_confidence'] = (\n            (old_conf_avg * (self.stats['total_commands'] - 1) + command.confidence) / \n            self.stats['total_commands']\n        )\n        \n        # Update language distribution\n        if command.language in self.stats['language_distribution']:\n            self.stats['language_distribution'][command.language] += 1\n        else:\n            self.stats['language_distribution'][command.language] = 1\n    \n    def start_continuous_listening(self, callback_func=None):\n        \"\"\"\n        Start continuous listening in a background thread\n        \n        :param callback_func: Function to call when a command is recognized\n        \"\"\"\n        if self.is_listening:\n            print(\"Already listening\")\n            return\n        \n        self.is_listening = True\n        self.listening_thread = threading.Thread(\n            target=self._continuous_listening_worker, \n            args=(callback_func,)\n        )\n        self.listening_thread.start()\n        \n        print(\"Started continuous listening\")\n    \n    def stop_continuous_listening(self):\n        \"\"\"Stop continuous listening\"\"\"\n        self.is_listening = False\n        if self.listening_thread:\n            self.listening_thread.join()\n        print(\"Stopped continuous listening\")\n    \n    def _continuous_listening_worker(self, callback_func):\n        \"\"\"\n        Worker function for continuous listening in background thread\n        \"\"\"\n        with sr.Microphone() as source:\n            print(\"Adjusting for ambient noise...\")\n            self.speech_recognizer.adjust_for_ambient_noise(source, duration=1)\n            print(\"Listening for voice commands...\")\n            \n            while self.is_listening:\n                try:\n                    # Listen for audio with timeout\n                    audio = self.speech_recognizer.listen(source, timeout=1, phrase_time_limit=5)\n                    \n                    # Convert to raw data for Whisper processing\n                    audio_buffer = audio.get_raw_data()\n                    \n                    # Process with Whisper\n                    command = self.transcribe_audio_buffer(audio_buffer)\n                    \n                    if command and command.text:\n                        print(f\"Heard: '{command.text}' (confidence: {command.confidence:.2f})\")\n                        \n                        # Call the callback if provided\n                        if callback_func:\n                            try:\n                                callback_func(command)\n                            except Exception as e:\n                                print(f\"Error in callback function: {e}\")\n                \n                except sr.WaitTimeoutError:\n                    # This is expected when there's no speech\n                    continue\n                except sr.UnknownValueError:\n                    print(\"Could not understand audio\")\n                    continue\n                except sr.RequestError as e:\n                    print(f\"Error with speech recognition: {e}\")\n                    continue\n                except Exception as e:\n                    print(f\"Unexpected error: {e}\")\n                    continue\n    \n    def get_statistics(self):\n        \"\"\"Get current statistics about voice recognition performance\"\"\"\n        return self.stats.copy()\n    \n    def reset_statistics(self):\n        \"\"\"Reset all statistics\"\"\"\n        self.stats = {\n            'total_commands': 0,\n            'avg_processing_time': 0.0,\n            'avg_confidence': 0.0,\n            'language_distribution': {}\n        }\n\n\nclass VoiceCommandProcessor:\n    \"\"\"\n    Processor for voice commands that integrates with Whisper\n    \"\"\"\n    def __init__(self):\n        self.whisper_listener = WhisperRobotListener()\n        self.command_history = []\n        self.confidence_threshold = 0.7  # Minimum confidence to accept command\n        self.understanding_phrases = [\n            \"I will\", \"Okay, I'll\", \"Sure, processing\", \"Got it, I'll\"\n        ]\n        self.error_phrases = [\n            \"I didn't understand\", \"Could you repeat\", \"I'm sorry, I didn't catch\"\n        ]\n    \n    def process_voice_command(self, audio_input):\n        \"\"\"\n        Process voice command from audio input and return processed result\n        \n        :param audio_input: Can be file path or audio buffer\n        :return: Processed command with validation and context\n        \"\"\"\n        if isinstance(audio_input, str):\n            # File path\n            command = self.whisper_listener.transcribe_audio_file(audio_input)\n        else:\n            # Audio buffer\n            command = self.whisper_listener.transcribe_audio_buffer(audio_input)\n        \n        if command is None:\n            return None\n        \n        # Validate command confidence\n        if command.confidence < self.confidence_threshold:\n            # Low confidence - request clarification\n            return {\n                'status': 'uncertain',\n                'command': command,\n                'feedback': f\"I heard '{command.text}' but I'm not confident (confidence: {command.confidence:.2f}). Could you repeat that?\"\n            }\n        \n        # High confidence - process normally\n        processed_result = self._process_valid_command(command)\n        \n        # Add to history\n        self.command_history.append(processed_result)\n        \n        return processed_result\n    \n    def _process_valid_command(self, command):\n        \"\"\"\n        Process a valid high-confidence command\n        \n        :param command: VoiceCommand object with high confidence\n        :return: Dictionary with processed command information\n        \"\"\"\n        # Here you would typically:\n        # 1. Parse the command to extract intent and entities\n        # 2. Validate against robot capabilities\n        # 3. Generate a robot action plan\n        # 4. Return appropriate response\n        \n        # Simple command parsing for demonstration\n        text_lower = command.text.lower()\n        intent = self._determine_intent(text_lower)\n        \n        # Extract relevant entities\n        entities = self._extract_entities(text_lower)\n        \n        result = {\n            'status': 'recognized',\n            'command': command,\n            'intent': intent,\n            'entities': entities,\n            'feedback': f\"I will {self._generate_response(intent, entities)}\",\n            'timestamp': time.time()\n        }\n        \n        return result\n    \n    def _determine_intent(self, text):\n        \"\"\"\n        Determine the intent of the voice command (simplified parser)\n        \"\"\"\n        if any(word in text for word in ['move', 'go', 'navigate', 'walk', 'come']):\n            return 'navigation'\n        elif any(word in text for word in ['pick', 'grasp', 'take', 'grab', 'lift']):\n            return 'manipulation'\n        elif any(word in text for word in ['turn', 'rotate', 'spin']):\n            return 'rotation'\n        elif any(word in text for word in ['stop', 'halt', 'pause']):\n            return 'stop'\n        elif any(word in text for word in ['follow', 'come after', ' accompany']):\n            return 'follow'\n        else:\n            return 'unknown'\n    \n    def _extract_entities(self, text):\n        \"\"\"\n        Extract relevant entities from the text (simplified extractor)\n        \"\"\"\n        entities = {}\n        \n        # Object recognition\n        objects = ['cup', 'box', 'ball', 'bottle', 'book', 'table', 'chair', 'door', 'window']\n        for obj in objects:\n            if obj in text:\n                if 'objects' not in entities:\n                    entities['objects'] = []\n                entities['objects'].append(obj)\n        \n        # Location recognition\n        locations = ['kitchen', 'living room', 'bedroom', 'office', 'table', 'counter', 'shelf']\n        for loc in locations:\n            if loc in text:\n                if 'locations' not in entities:\n                    entities['locations'] = []\n                entities['locations'].append(loc)\n        \n        # Direction recognition\n        directions = ['left', 'right', 'forward', 'backward', 'up', 'down']\n        for dir in directions:\n            if dir in text:\n                if 'directions' not in entities:\n                    entities['directions'] = []\n                entities['directions'].append(dir)\n        \n        # Distance recognition (simplified)\n        distances = ['close', 'near', 'far', 'behind', 'in front', 'next to']\n        for dist in distances:\n            if dist in text:\n                if 'distances' not in entities:\n                    entities['distances'] = []\n                entities['distances'].append(dist)\n        \n        return entities\n    \n    def _generate_response(self, intent, entities):\n        \"\"\"\n        Generate a verbal response based on intent and entities\n        \"\"\"\n        if intent == 'navigation':\n            if entities.get('locations'):\n                return f\"move to the {entities['locations'][0]}\"\n            elif entities.get('directions'):\n                return f\"move to the {entities['directions'][0]}\"\n            else:\n                return \"move to the requested location\"\n        elif intent == 'manipulation':\n            if entities.get('objects'):\n                return f\"grasp the {entities['objects'][0]}\"\n            else:\n                return \"perform the requested manipulation\"\n        elif intent == 'rotation':\n            if entities.get('directions'):\n                return f\"turn to the {entities['directions'][0]}\"\n            else:\n                return \"rotate as requested\"\n        else:\n            return \"execute the command\"\n    \n    def start_robot_assistant(self, callback_func=None):\n        \"\"\"\n        Start the robot assistant that listens and processes voice commands\n        \"\"\"\n        def whisper_callback(command):\n            if callback_func:\n                # Process the command and call user callback\n                result = self._process_valid_command(command)\n                callback_func(result)\n        \n        print(\"Starting robot voice assistant...\")\n        self.whisper_listener.start_continuous_listening(whisper_callback)\n    \n    def stop_robot_assistant(self):\n        \"\"\"Stop the robot assistant\"\"\"\n        self.whisper_listener.stop_continuous_listening()\n        print(\"Stopped robot voice assistant\")\n\n\ndef main():\n    \"\"\"Example usage of Whisper integration for robotics\"\"\"\n    print(\"Whisper Voice Recognition for Robotics Example\")\n    \n    # Initialize the voice command processor\n    processor = VoiceCommandProcessor()\n    \n    # Example 1: Process a pre-recorded command (simulated)\n    print(\"\\n--- Example 1: Processing audio command ---\")\n    \n    # Instead of actual audio, we'll simulate by creating a mock command\n    # In practice, you'd call processor.process_voice_command(audio_file_path)\n    mock_command = VoiceCommand(\n        text=\"Please go to the kitchen and bring me a cup\",\n        confidence=0.85,\n        timestamp=time.time(),\n        language=\"en\",\n        audio_duration=3.5\n    )\n    \n    # Process the mock command\n    result = {\n        'status': 'recognized',\n        'command': mock_command,\n        'intent': processor._determine_intent(mock_command.text.lower()),\n        'entities': processor._extract_entities(mock_command.text.lower()),\n        'feedback': f\"I will {processor._generate_response(processor._determine_intent(mock_command.text.lower()), processor._extract_entities(mock_command.text.lower()))}\",\n        'timestamp': time.time()\n    }\n    \n    print(f\"Recognized command: '{mock_command.text}'\")\n    print(f\"Intent: {result['intent']}\")\n    print(f\"Entities: {result['entities']}\")\n    print(f\"Feedback: {result['feedback']}\")\n    \n    # Example 2: Show statistics\n    print(\"\\n--- Example 2: Voice recognition statistics ---\")\n    stats = processor.whisper_listener.get_statistics()\n    print(f\"Total commands processed: {stats['total_commands']}\")\n    print(f\"Average processing time: {stats['avg_processing_time']:.3f}s\")\n    print(f\"Average confidence: {stats['avg_confidence']:.3f}\")\n    print(f\"Language distribution: {stats['language_distribution']}\")\n    \n    # Example 3: Simulate robot assistant\n    print(\"\\n--- Example 3: Robot assistant simulation ---\")\n    \n    def command_callback(result):\n        print(f\"Robot processing: '{result['command'].text}'\")\n        print(f\"Intent: {result['intent']}, Confidence: {result['command'].confidence:.2f}\")\n    \n    # In a real system, this would start continuous listening\n    # processor.start_robot_assistant(command_callback)\n    \n    # For this example, just show the available functionality\n    print(\"Robot assistant functionality ready\")\n    print(\"Commands will be processed as they are recognized\")\n    \n    print(\"\\nWhisper voice recognition integration example completed\")\n\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"simulation-demonstration",children:"Simulation Demonstration"}),"\n",(0,s.jsx)(n.p,{children:"This implementation demonstrates how Whisper can be integrated into a robotics system for voice recognition. The system handles audio preprocessing, transcription, confidence estimation, and integration with downstream language processing. The code can be combined with ROS 2 to create voice-controlled robotic systems."}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-lab-whisper-integration-for-robotics",children:"Hands-On Lab: Whisper Integration for Robotics"}),"\n",(0,s.jsx)(n.p,{children:"In this lab, you'll implement and test Whisper integration with a robotic system:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Set up Whisper model for robotic voice recognition"}),"\n",(0,s.jsx)(n.li,{children:"Implement audio preprocessing pipeline"}),"\n",(0,s.jsx)(n.li,{children:"Test Whisper performance with various audio conditions"}),"\n",(0,s.jsx)(n.li,{children:"Integrate Whisper outputs with robotic action planning"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the system's robustness to noise and accents"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"required-equipment",children:"Required Equipment:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ROS 2 Humble environment"}),"\n",(0,s.jsx)(n.li,{children:"CUDA-compatible GPU (for efficient Whisper processing)"}),"\n",(0,s.jsx)(n.li,{children:"Microphone for audio input"}),"\n",(0,s.jsxs)(n.li,{children:["Whisper models installed (",(0,s.jsx)(n.code,{children:"pip install openai-whisper"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:["Speech Recognition library (",(0,s.jsx)(n.code,{children:"pip install SpeechRecognition"}),")"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"instructions",children:"Instructions:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Create a new ROS 2 package: ",(0,s.jsx)(n.code,{children:"ros2 pkg create --build-type ament_python whisper_robotic_control"})]}),"\n",(0,s.jsx)(n.li,{children:"Implement the WhisperRobotListener and VoiceCommandProcessor classes"}),"\n",(0,s.jsx)(n.li,{children:"Create a launch file to start the voice recognition node"}),"\n",(0,s.jsx)(n.li,{children:"Test with different voice commands in a quiet environment"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate system performance with varying audio quality"}),"\n",(0,s.jsx)(n.li,{children:"Implement confidence-based command validation"}),"\n",(0,s.jsx)(n.li,{children:"Integrate with a robot simulator (e.g., TurtleBot3) for real testing"}),"\n",(0,s.jsx)(n.li,{children:"Document the recognition accuracy and response times"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"common-pitfalls--debugging-notes",children:"Common Pitfalls & Debugging Notes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Quality"}),": Whisper performance degrades significantly with poor audio quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Computational Resources"}),": Whisper models can be computationally intensive; select appropriate model size"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency"}),": Real-time applications require careful consideration of processing time"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Microphone Setup"}),": Proper microphone placement and audio input configuration are crucial"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Background Noise"}),": Implement noise filtering if operating in noisy environments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Settings"}),": Ensure Whisper model matches the expected language"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Memory Management"}),": Large Whisper models consume significant memory"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary--key-terms",children:"Summary & Key Terms"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Terms:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Whisper"}),": OpenAI's automatic speech recognition system"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"}),": Technology that converts speech to text"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transformer Architecture"}),": Neural network architecture used in Whisper"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Confidence Scoring"}),": Measure of reliability for ASR outputs"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Audio Preprocessing"}),": Steps to prepare audio for ASR processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Streaming ASR"}),": Real-time speech recognition on audio streams"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Identification"}),": Determining the language of spoken input"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"further-reading--citations",children:"Further Reading & Citations"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'Radford, A., et al. (2022). "Robust Speech Recognition via Large-Scale Weak Supervision." arXiv preprint arXiv:2212.04356.'}),"\n",(0,s.jsxs)(n.li,{children:['OpenAI. (2022). "Introducing Whisper." OpenAI Blog. ',(0,s.jsx)(n.a,{href:"https://openai.com/research/whisper",children:"https://openai.com/research/whisper"})]}),"\n",(0,s.jsx)(n.li,{children:'Zhang, Y., et al. (2022). "Transformer-Based Acoustic Modeling for Speech Recognition." IEEE Signal Processing Magazine.'}),"\n",(0,s.jsx)(n.li,{children:'Hori, T., et al. (2022). "End-to-End Speech Recognition and Understanding with Transformers." IEEE International Conference on Acoustics, Speech and Signal Processing.'}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Explain how Whisper's transformer architecture differs from traditional speech recognition systems."}),"\n",(0,s.jsx)(n.li,{children:"What are the computational requirements for running different Whisper model sizes?"}),"\n",(0,s.jsx)(n.li,{children:"Describe how to preprocess audio data for optimal Whisper performance in robotics."}),"\n",(0,s.jsx)(n.li,{children:"How can confidence scores from Whisper be used to improve robotic command execution?"}),"\n",(0,s.jsx)(n.li,{children:"What are the key challenges in deploying Whisper for real-time robotic applications?"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous"}),": ",(0,s.jsx)(n.a,{href:"/Hackthon_SpecKitPlus/docs/vla-robotics/intro",children:"Introduction to Vision-Language-Action Robotics"}),(0,s.jsx)(n.br,{}),"\n",(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"/Hackthon_SpecKitPlus/docs/vla-robotics/llm-planning",children:"LLM-Based Planning for Robotics"})]})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const s={},o=t.createContext(s);function r(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);