"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[759],{5994:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"simulation/digital-twins","title":"Digital Twins and Sensor Simulation","description":"Digital twins in robotics represent virtual replicas of physical systems that mirror their real-world counterparts\' behaviors, characteristics, and states in real-time. Combined with accurate sensor simulation, digital twins provide an invaluable platform for testing, validating, and training robotic systems before deployment in the physical world.","source":"@site/docs/03-simulation/digital-twins.md","sourceDirName":"03-simulation","slug":"/simulation/digital-twins","permalink":"/Hackthon_SpecKitPlus/docs/simulation/digital-twins","draft":false,"unlisted":false,"editUrl":"https://github.com/shaheryarshah/Hackthon_SpecKitPlus/edit/main/docs/docs/03-simulation/digital-twins.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Gazebo Physics Simulation","permalink":"/Hackthon_SpecKitPlus/docs/simulation/gazebo"},"next":{"title":"Introduction to NVIDIA Isaac Platform","permalink":"/Hackthon_SpecKitPlus/docs/isaac-platform/intro"}}');var a=i(4848),t=i(8453);const r={},l="Digital Twins and Sensor Simulation",o={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Digital Twin Architecture",id:"digital-twin-architecture",level:3},{value:"Sensor Simulation Fidelity",id:"sensor-simulation-fidelity",level:3},{value:"Reality Gap",id:"reality-gap",level:3},{value:"Equations and Models",id:"equations-and-models",level:2},{value:"Digital Twin State Synchronization",id:"digital-twin-state-synchronization",level:3},{value:"Sensor Simulation Model with Noise",id:"sensor-simulation-model-with-noise",level:3},{value:"Code Example: Sensor Simulation with Noise",id:"code-example-sensor-simulation-with-noise",level:2},{value:"Simulation Demonstration",id:"simulation-demonstration",level:2},{value:"Hands-On Lab: Advanced Sensor Simulation",id:"hands-on-lab-advanced-sensor-simulation",level:2},{value:"Required Equipment:",id:"required-equipment",level:3},{value:"Instructions:",id:"instructions",level:3},{value:"Common Pitfalls &amp; Debugging Notes",id:"common-pitfalls--debugging-notes",level:2},{value:"Summary &amp; Key Terms",id:"summary--key-terms",level:2},{value:"Further Reading &amp; Citations",id:"further-reading--citations",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2}];function c(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"digital-twins-and-sensor-simulation",children:"Digital Twins and Sensor Simulation"})}),"\n",(0,a.jsx)(n.p,{children:"Digital twins in robotics represent virtual replicas of physical systems that mirror their real-world counterparts' behaviors, characteristics, and states in real-time. Combined with accurate sensor simulation, digital twins provide an invaluable platform for testing, validating, and training robotic systems before deployment in the physical world."}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"After completing this section, you should be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Define and implement digital twin concepts for robotic systems"}),"\n",(0,a.jsx)(n.li,{children:"Create accurate sensor simulation models in both Gazebo and Unity"}),"\n",(0,a.jsx)(n.li,{children:"Understand the benefits and limitations of digital twin technologies"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate simulation-to-reality transfer approaches"}),"\n",(0,a.jsx)(n.li,{children:"Design validation strategies for digital twin systems"}),"\n",(0,a.jsx)(n.li,{children:"Identify the fidelity requirements for different robotic applications"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,a.jsx)(n.h3,{id:"digital-twin-architecture",children:"Digital Twin Architecture"}),"\n",(0,a.jsx)(n.p,{children:"A digital twin architecture for robotics typically consists of:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Physical System"}),": The real-world robot and environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Virtual Model"}),": Digital replica of the physical system"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Connection"}),": Real-time or batch data flow between physical and virtual systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulation Engine"}),": Physics and sensor simulation capabilities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Analytics Layer"}),": For monitoring, prediction, and optimization"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-simulation-fidelity",children:"Sensor Simulation Fidelity"}),"\n",(0,a.jsx)(n.p,{children:"Achieving realistic sensor simulation requires modeling:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Noise"}),": Realistic noise models based on physical sensor properties"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environmental Conditions"}),": Effects of lighting, weather, dust, etc."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Motion Artifacts"}),": Effects of robot motion on sensor data"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cross-Sensor Effects"}),": Interactions between different sensor modalities"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"reality-gap",children:"Reality Gap"}),"\n",(0,a.jsx)(n.p,{children:'The "reality gap" refers to the differences between simulation and real-world performance. Key components include:'}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"System Modeling Errors"}),": Inaccuracies in physical modeling"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Simulation Errors"}),": Differences in sensor behavior"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environmental Differences"}),": Simplifications in the simulated environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Unmodeled Dynamics"}),": Physical effects not captured in simulation"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"equations-and-models",children:"Equations and Models"}),"\n",(0,a.jsx)(n.h3,{id:"digital-twin-state-synchronization",children:"Digital Twin State Synchronization"}),"\n",(0,a.jsx)(n.p,{children:"The state of the digital twin (x_sim) should approximate the real system state (x_real):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"x_sim(t) \u2248 x_real(t)\n"})}),"\n",(0,a.jsx)(n.p,{children:"With synchronization achieved through:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"dx_sim/dt = f_sim(u, x_sim, t)\ndx_real/dt = f_real(u, x_real, t)\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Where ",(0,a.jsx)(n.code,{children:"f_sim"})," and ",(0,a.jsx)(n.code,{children:"f_real"})," are the system dynamics models and ",(0,a.jsx)(n.code,{children:"u"})," is the control input."]}),"\n",(0,a.jsx)(n.h3,{id:"sensor-simulation-model-with-noise",children:"Sensor Simulation Model with Noise"}),"\n",(0,a.jsx)(n.p,{children:"For realistic sensor simulation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"z_sim = h(x_sim) + n_system + n_environment\n"})}),"\n",(0,a.jsx)(n.p,{children:"Where:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"z_sim"})," is the simulated sensor reading"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"h(x_sim)"})," is the noiseless sensor reading based on state"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"n_system"})," is system-specific sensor noise"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"n_environment"})," is environment-dependent noise"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"code-example-sensor-simulation-with-noise",children:"Code Example: Sensor Simulation with Noise"}),"\n",(0,a.jsx)(n.p,{children:"Here's an example of implementing realistic sensor simulation with noise:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import numpy as np\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport cv2\n\n\nclass SensorSimulatorNode(Node):\n    def __init__(self):\n        super().__init__('sensor_simulator')\n        \n        # Publishers for simulated sensors\n        self.lidar_publisher = self.create_publisher(LaserScan, 'scan', 10)\n        self.camera_publisher = self.create_publisher(Image, 'camera/image_raw', 10)\n        \n        # CV Bridge for image conversion\n        self.bridge = CvBridge()\n        \n        # Simulation parameters\n        self.lidar_angle_min = -np.pi/2\n        self.lidar_angle_max = np.pi/2\n        self.lidar_angle_increment = np.pi / 180  # 1 degree\n        self.lidar_range_min = 0.1\n        self.lidar_range_max = 30.0\n        \n        # Camera parameters\n        self.camera_width = 640\n        self.camera_height = 480\n        self.camera_fov = 90  # degrees\n        \n        # Timer for sensor simulation\n        self.timer = self.create_timer(0.1, self.publish_sensors)\n        \n        # Simulate robot moving in a world with obstacles\n        self.robot_x = 0.0\n        self.robot_y = 0.0\n        self.robot_theta = 0.0\n        \n        # Simulated obstacles in the environment\n        self.obstacles = [\n            {'x': 5.0, 'y': 2.0, 'radius': 0.5},  # Obstacle 1\n            {'x': 3.0, 'y': -1.0, 'radius': 0.8},  # Obstacle 2\n            {'x': 7.0, 'y': 0.0, 'radius': 1.0},   # Obstacle 3\n        ]\n    \n    def simulate_lidar_scan(self):\n        \"\"\"Simulate LIDAR scan with realistic noise\"\"\"\n        num_readings = int((self.lidar_angle_max - self.lidar_angle_min) / self.lidar_angle_increment) + 1\n        ranges = []\n        \n        for i in range(num_readings):\n            # Calculate angle of this ray in robot frame\n            angle = self.lidar_angle_min + i * self.lidar_angle_increment\n            \n            # Transform to global frame\n            global_angle = self.robot_theta + angle\n            \n            # Calculate ray endpoint\n            max_range = self.lidar_range_max\n            ray_x = self.robot_x + max_range * np.cos(global_angle)\n            ray_y = self.robot_y + max_range * np.sin(global_angle)\n            \n            # Check for intersections with obstacles\n            min_distance = self.lidar_range_max\n            \n            for obstacle in self.obstacles:\n                # Calculate distance from ray start to obstacle center\n                dx = obstacle['x'] - self.robot_x\n                dy = obstacle['y'] - self.robot_y\n                \n                # Calculate projection of obstacle center onto ray\n                ray_length = np.sqrt(dx*dx + dy*dy)\n                ray_angle_to_obstacle = np.arctan2(dy, dx) - global_angle\n                \n                # Correct for angle wrapping\n                ray_angle_to_obstacle = (ray_angle_to_obstacle + np.pi) % (2*np.pi) - np.pi\n                \n                # If obstacle is roughly in direction of ray\n                if abs(ray_angle_to_obstacle) < 0.1:  # 0.1 radian = ~5.7 degrees\n                    # Calculate closest point on ray to obstacle center\n                    distance_to_obstacle = ray_length * np.cos(ray_angle_to_obstacle)\n                    \n                    # Perpendicular distance from ray to obstacle center\n                    perpendicular_distance = ray_length * np.abs(np.sin(ray_angle_to_obstacle))\n                    \n                    # If ray passes close enough to obstacle\n                    if perpendicular_distance <= obstacle['radius']:\n                        # Calculate where ray intersects obstacle\n                        intersection_distance = distance_to_obstacle - np.sqrt(obstacle['radius']**2 - perpendicular_distance**2)\n                        \n                        if 0 < intersection_distance < min_distance:\n                            min_distance = intersection_distance\n            \n            # Add realistic sensor noise\n            noise_std = 0.02  # 2cm standard deviation\n            noisy_distance = min_distance + np.random.normal(0, noise_std)\n            \n            # Apply sensor range limits\n            if noisy_distance < self.lidar_range_min:\n                ranges.append(float('inf'))  # Out of range low\n            elif noisy_distance > self.lidar_range_max:\n                ranges.append(float('inf'))  # Out of range high\n            else:\n                ranges.append(noisy_distance)\n        \n        return ranges\n    \n    def simulate_camera_image(self):\n        \"\"\"Simulate camera image with realistic rendering\"\"\"\n        # Create a simulated image using OpenCV\n        image = np.ones((self.camera_height, self.camera_width, 3), dtype=np.uint8) * 150  # Gray background\n        \n        # Simulate obstacles as colored circles\n        for obstacle in self.obstacles:\n            # Transform obstacle position to robot frame\n            dx = obstacle['x'] - self.robot_x\n            dy = obstacle['y'] - self.robot_y\n            \n            # Convert to image coordinates (simplified pinhole model)\n            # For a 90-degree FOV centered on robot's heading\n            angle_to_obstacle = np.arctan2(dy, dx) - self.robot_theta\n            \n            # Only render if obstacle is in front of robot\n            if abs(angle_to_obstacle) <= np.pi/2:  # 90 degree FOV\n                distance = np.sqrt(dx*dx + dy*dy)\n                \n                # Calculate image position (simplified)\n                if distance > 0:\n                    pixel_x = int(self.camera_width/2 + (angle_to_obstacle / (np.pi/2)) * self.camera_width/2)\n                    pixel_y = int(self.camera_height/2)\n                    \n                    # Calculate approximate size based on distance\n                    size = max(5, int(50 / distance))  # Closer objects appear larger\n                    \n                    if 0 <= pixel_x < self.camera_width:\n                        # Draw obstacle as a colored circle\n                        color = (0, 255, 0) if 'radius' in obstacle and obstacle['radius'] < 0.7 else (0, 100, 255)\n                        image = cv2.circle(image, (pixel_x, pixel_y), size, color, -1)\n        \n        # Add realistic image noise\n        noise = np.random.normal(0, 10, image.shape).astype(np.int16)\n        image = np.clip(image.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n        \n        return image\n    \n    def publish_sensors(self):\n        \"\"\"Publish simulated sensor data with realistic noise\"\"\"\n        # Update robot position (simple movement model)\n        self.robot_x += 0.1 * np.cos(self.robot_theta)  # Move forward slightly\n        self.robot_theta += 0.05  # Rotate slowly\n        \n        # Create and publish LIDAR scan\n        scan_msg = LaserScan()\n        scan_msg.header = Header()\n        scan_msg.header.stamp = self.get_clock().now().to_msg()\n        scan_msg.header.frame_id = 'laser_frame'\n        \n        scan_msg.angle_min = self.lidar_angle_min\n        scan_msg.angle_max = self.lidar_angle_max\n        scan_msg.angle_increment = self.lidar_angle_increment\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1\n        scan_msg.range_min = self.lidar_range_min\n        scan_msg.range_max = self.lidar_range_max\n        \n        scan_msg.ranges = self.simulate_lidar_scan()\n        self.lidar_publisher.publish(scan_msg)\n        \n        # Create and publish camera image\n        image = self.simulate_camera_image()\n        image_msg = self.bridge.cv2_to_imgmsg(image, encoding='bgr8')\n        image_msg.header = Header()\n        image_msg.header.stamp = self.get_clock().now().to_msg()\n        image_msg.header.frame_id = 'camera_frame'\n        self.camera_publisher.publish(image_msg)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    sensor_simulator = SensorSimulatorNode()\n    \n    try:\n        rclpy.spin(sensor_simulator)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        sensor_simulator.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"simulation-demonstration",children:"Simulation Demonstration"}),"\n",(0,a.jsx)(n.p,{children:"This node simulates both LIDAR and camera sensors with realistic noise models. The simulated robot navigates through an environment with obstacles, and the sensors detect these obstacles with appropriate noise and limitations, similar to real sensors."}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-lab-advanced-sensor-simulation",children:"Hands-On Lab: Advanced Sensor Simulation"}),"\n",(0,a.jsx)(n.p,{children:"In this lab, you'll implement and validate advanced sensor simulation:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Create realistic sensor simulators with noise models"}),"\n",(0,a.jsx)(n.li,{children:"Validate the simulators against real sensor data"}),"\n",(0,a.jsx)(n.li,{children:"Compare different noise modeling approaches"}),"\n",(0,a.jsx)(n.li,{children:"Analyze the impact of sensor fidelity on robotic tasks"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"required-equipment",children:"Required Equipment:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"ROS 2 Humble environment"}),"\n",(0,a.jsx)(n.li,{children:"Python development environment"}),"\n",(0,a.jsx)(n.li,{children:"Basic understanding of probability and statistics"}),"\n",(0,a.jsx)(n.li,{children:"(Optional) Real sensor data for comparison"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"instructions",children:"Instructions:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:["Create a new ROS 2 package: ",(0,a.jsx)(n.code,{children:"ros2 pkg create --build-type ament_python advanced_sensor_simulator"})]}),"\n",(0,a.jsx)(n.li,{children:"Add the SensorSimulatorNode code to your package"}),"\n",(0,a.jsx)(n.li,{children:"Create launch files to run the simulator with visualization tools"}),"\n",(0,a.jsx)(n.li,{children:"Add additional sensor types (e.g., IMU, GPS) with appropriate noise models"}),"\n",(0,a.jsx)(n.li,{children:"Implement parameterization of noise characteristics"}),"\n",(0,a.jsx)(n.li,{children:"Test the simulator with different environmental conditions"}),"\n",(0,a.jsx)(n.li,{children:"Visualize the sensor data in RViz"}),"\n",(0,a.jsx)(n.li,{children:"Document how different noise parameters affect robot perception"}),"\n",(0,a.jsx)(n.li,{children:"(Optional) Compare simulated data with real sensor data if available"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"common-pitfalls--debugging-notes",children:"Common Pitfalls & Debugging Notes"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Noise Modeling"}),": Ensure noise models are realistic but not overly complex"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Computational Load"}),": Advanced sensor simulation can be computationally expensive"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Parameter Tuning"}),": Carefully tune noise parameters based on real sensor specifications"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environmental Fidelity"}),": Balance environmental complexity with computational efficiency"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Validation"}),": Validate sensor simulators against real sensor data when possible"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary--key-terms",children:"Summary & Key Terms"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Key Terms:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Digital Twin"}),": Virtual replica of a physical system that mirrors its state and behavior"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Simulation"}),": Modeling of real-world sensors with realistic noise and limitations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reality Gap"}),": Differences between simulated and real-world robot performance"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"System Modeling Errors"}),": Inaccuracies in the physical model of the system"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Fidelity"}),": Accuracy of the simulated sensor compared to the real sensor"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simulation Validation"}),": Process of verifying that simulation accurately represents reality"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception Simulation"}),": Modeling of robot sensory inputs with realistic properties"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"further-reading--citations",children:"Further Reading & Citations"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Rasheed, A., San, O., & Kvamsdal, T. (2020). Digital twin: Values, challenges and enablers from a modeling perspective. IEEE Access, 8, 21980-22012."}),"\n",(0,a.jsx)(n.li,{children:"Batty, M. (2018). Digital twins. Environment and Planning B: Urban Analytics and City Science, 45(5), 817-820."}),"\n",(0,a.jsx)(n.li,{children:"Kerschbaum, S., et al. (2021). Digital twin in manufacturing: A categorical literature review and classification."}),"\n",(0,a.jsx)(n.li,{children:"Khajavi, G. H., et al. (2019). Additive manufacturing in the spare parts supply chain. Computers & Industrial Engineering."}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Define digital twins and explain their role in robotics development."}),"\n",(0,a.jsx)(n.li,{children:"What are the main components of a digital twin architecture for robotics?"}),"\n",(0,a.jsx)(n.li,{children:"Describe how sensor noise is modeled in realistic sensor simulation."}),"\n",(0,a.jsx)(n.li,{children:'What is the "reality gap" and how can it be minimized?'}),"\n",(0,a.jsx)(n.li,{children:"Compare the computational requirements of high-fidelity vs. low-fidelity sensor simulation."}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Previous"}),": ",(0,a.jsx)(n.a,{href:"./unity.md",children:"Unity for Robotics Visualization"}),(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Next"}),": ",(0,a.jsx)(n.a,{href:"/Hackthon_SpecKitPlus/docs/isaac-platform/intro",children:"NVIDIA Isaac Platform - Introduction"})]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var s=i(6540);const a={},t=s.createContext(a);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);