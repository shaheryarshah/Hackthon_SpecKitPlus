"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[134],{8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var i=t(6540);const r={},s=i.createContext(r);function a(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),i.createElement(s.Provider,{value:n},e.children)}},8620:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"appendices/reinforcement-learning","title":"Appendix B: Reinforcement Learning for Robot Control","description":"Reinforcement Learning (RL) has emerged as a powerful paradigm for robot control, enabling robots to learn complex behaviors through interaction with their environment. This appendix covers the fundamentals of RL for robotics, implementation techniques, simulation-to-reality transfer, and practical applications. RL is particularly valuable for robotics because it can handle high-dimensional continuous state and action spaces, and can learn behaviors that are difficult to program explicitly.","source":"@site/docs/08-appendices/reinforcement-learning.md","sourceDirName":"08-appendices","slug":"/appendices/reinforcement-learning","permalink":"/docs/appendices/reinforcement-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/shaheryarshah/Hackthon_SpecKitPlus/edit/main/docs/docs/08-appendices/reinforcement-learning.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Appendix A: Hardware Requirements and Lab Options","permalink":"/docs/appendices/hardware"},"next":{"title":"Appendix C: Sim-to-Real Transfer Techniques","permalink":"/docs/appendices/sim-to-real"}}');var r=t(4848),s=t(8453);const a={},o="Appendix B: Reinforcement Learning for Robot Control",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Markov Decision Process (MDP) Formulation",id:"markov-decision-process-mdp-formulation",level:3},{value:"Continuous Control Challenges",id:"continuous-control-challenges",level:3},{value:"Exploration vs. Exploitation",id:"exploration-vs-exploitation",level:3},{value:"Equations and Models",id:"equations-and-models",level:2},{value:"Bellman Equation",id:"bellman-equation",level:3},{value:"Policy Gradient Theorem",id:"policy-gradient-theorem",level:3},{value:"Deep Deterministic Policy Gradient (DDPG)",id:"deep-deterministic-policy-gradient-ddpg",level:3},{value:"Code Example: Deep Deterministic Policy Gradient for Robot Control",id:"code-example-deep-deterministic-policy-gradient-for-robot-control",level:2},{value:"Simulation Demonstration",id:"simulation-demonstration",level:2},{value:"Hands-On Lab: Reinforcement Learning for Robot Control",id:"hands-on-lab-reinforcement-learning-for-robot-control",level:2},{value:"Required Equipment:",id:"required-equipment",level:3},{value:"Instructions:",id:"instructions",level:3},{value:"Common Pitfalls &amp; Debugging Notes",id:"common-pitfalls--debugging-notes",level:2},{value:"Summary &amp; Key Terms",id:"summary--key-terms",level:2},{value:"Further Reading &amp; Citations",id:"further-reading--citations",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"appendix-b-reinforcement-learning-for-robot-control",children:"Appendix B: Reinforcement Learning for Robot Control"})}),"\n",(0,r.jsx)(n.p,{children:"Reinforcement Learning (RL) has emerged as a powerful paradigm for robot control, enabling robots to learn complex behaviors through interaction with their environment. This appendix covers the fundamentals of RL for robotics, implementation techniques, simulation-to-reality transfer, and practical applications. RL is particularly valuable for robotics because it can handle high-dimensional continuous state and action spaces, and can learn behaviors that are difficult to program explicitly."}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"After studying this appendix, you should be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Understand the fundamentals of reinforcement learning in robotic contexts"}),"\n",(0,r.jsx)(n.li,{children:"Implement basic RL algorithms for robot control tasks"}),"\n",(0,r.jsx)(n.li,{children:"Design reward functions for robotic manipulation and navigation"}),"\n",(0,r.jsx)(n.li,{children:"Apply simulation-to-reality transfer techniques for RL policies"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate the performance and safety of RL-based control systems"}),"\n",(0,r.jsx)(n.li,{children:"Understand the limitations and challenges of RL in robotics"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,r.jsx)(n.h3,{id:"markov-decision-process-mdp-formulation",children:"Markov Decision Process (MDP) Formulation"}),"\n",(0,r.jsx)(n.p,{children:"Robotic control problems are often formulated as MDPs:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"States (S)"}),": Robot configuration, sensor readings, environment state"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Actions (A)"}),": Joint commands, velocity commands, or high-level actions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rewards (R)"}),": Scalar feedback guiding learning (e.g., reaching, avoiding obstacles)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transitions (P)"}),": State transition probabilities (or deterministic in robotics)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"continuous-control-challenges",children:"Continuous Control Challenges"}),"\n",(0,r.jsx)(n.p,{children:"RL in robotics faces unique challenges:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Continuous Action Spaces"}),": Most robotic tasks require continuous control"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety Constraints"}),": Actions must be safe for robot and environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sample Efficiency"}),": Physical robots have limited trial-and-error budget"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time Requirements"}),": Control decisions in dynamic environments"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exploration-vs-exploitation",children:"Exploration vs. Exploitation"}),"\n",(0,r.jsx)(n.p,{children:"Critical trade-off in RL:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exploration"}),": Trying new actions to discover better policies"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exploitation"}),": Using known good actions to maximize rewards"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Curriculum Learning"}),": Gradually increasing task complexity"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"equations-and-models",children:"Equations and Models"}),"\n",(0,r.jsx)(n.h3,{id:"bellman-equation",children:"Bellman Equation"}),"\n",(0,r.jsx)(n.p,{children:"The optimal value function in reinforcement learning:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"V*(s) = max_a \u03a3_s' P(s'|s, a) [R(s, a, s') + \u03b3V*(s')]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"V*(s)"})," is the optimal value of state s"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"\u03b3"})," is the discount factor (0 \u2264 \u03b3 < 1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"P(s'|s, a)"})," is the probability of transitioning to state s' from state s with action a"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"R(s, a, s')"})," is the reward for the transition"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"policy-gradient-theorem",children:"Policy Gradient Theorem"}),"\n",(0,r.jsx)(n.p,{children:"For policy optimization in continuous action spaces:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u2207J(\u03b8) = E[\u2207log \u03c0_\u03b8(a|s) Q_\u03c0(s, a)]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"J(\u03b8)"})," is the expected return under policy \u03c0_\u03b8"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"\u03c0_\u03b8(a|s)"})," is the policy parameterized by \u03b8"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"Q_\u03c0(s, a)"})," is the action-value function"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"deep-deterministic-policy-gradient-ddpg",children:"Deep Deterministic Policy Gradient (DDPG)"}),"\n",(0,r.jsx)(n.p,{children:"Actor-critic algorithm for continuous control:"}),"\n",(0,r.jsx)(n.p,{children:"Actor update:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"\u2207_\u03b8^\u03bc J \u2248 E[\u2207_\u03b8^\u03bc \u03bc_\u03b8(s) \u2207_a Q(s, a)|s=s_t, a=\u03bc_\u03b8(s)]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Critic update:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"L = E[(r + \u03b3Q'(s', \u03bc'(s')) - Q(s, a))\xb2]\n"})}),"\n",(0,r.jsx)(n.p,{children:"Where:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"\u03bc_\u03b8(s)"})," is the actor (policy) network"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"Q(s, a)"})," is the critic (value) network"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"Q'"})," and ",(0,r.jsx)(n.code,{children:"\u03bc'"})," are target networks"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"code-example-deep-deterministic-policy-gradient-for-robot-control",children:"Code Example: Deep Deterministic Policy Gradient for Robot Control"}),"\n",(0,r.jsx)(n.p,{children:"Here's an implementation of DDPG for robotic control:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport random\nfrom collections import deque, namedtuple\nimport gym\nfrom torch.distributions import Normal\n\n\n# Experience replay buffer\nExperience = namedtuple(\'Experience\', [\'state\', \'action\', \'reward\', \'next_state\', \'done\'])\n\n\nclass ReplayBuffer:\n    """Replay buffer for storing and sampling experiences"""\n    \n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n    \n    def push(self, state, action, reward, next_state, done):\n        """Add experience to buffer"""\n        self.buffer.append(Experience(state, action, reward, next_state, done))\n    \n    def sample(self, batch_size):\n        """Sample batch of experiences"""\n        batch = random.sample(self.buffer, batch_size)\n        states = torch.stack([torch.FloatTensor(e.state) for e in batch])\n        actions = torch.stack([torch.FloatTensor(e.action) for e in batch])\n        rewards = torch.stack([torch.FloatTensor([e.reward]) for e in batch])\n        next_states = torch.stack([torch.FloatTensor(e.next_state) for e in batch])\n        dones = torch.stack([torch.FloatTensor([e.done]) for e in batch])\n        \n        return states, actions, rewards, next_states, dones\n    \n    def __len__(self):\n        return len(self.buffer)\n\n\nclass Actor(nn.Module):\n    """Actor network for policy"""\n    \n    def __init__(self, state_dim, action_dim, max_action):\n        super(Actor, self).__init__()\n        \n        self.l1 = nn.Linear(state_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, action_dim)\n        \n        self.max_action = max_action\n    \n    def forward(self, state):\n        """Forward pass to get action"""\n        a = F.relu(self.l1(state))\n        a = F.relu(self.l2(a))\n        a = torch.tanh(self.l3(a))  # Tanh to bound actions\n        return self.max_action * a\n\n\nclass Critic(nn.Module):\n    """Critic network for value estimation"""\n    \n    def __init__(self, state_dim, action_dim):\n        super(Critic, self).__init__()\n        \n        # Q1\n        self.l1 = nn.Linear(state_dim + action_dim, 256)\n        self.l2 = nn.Linear(256, 256)\n        self.l3 = nn.Linear(256, 1)\n        \n        # Q2\n        self.l4 = nn.Linear(state_dim + action_dim, 256)\n        self.l5 = nn.Linear(256, 256)\n        self.l6 = nn.Linear(256, 1)\n    \n    def forward(self, state, action):\n        """Forward pass to get Q-value"""\n        sa = torch.cat([state, action], 1)\n        \n        q1 = F.relu(self.l1(sa))\n        q1 = F.relu(self.l2(q1))\n        q1 = self.l3(q1)\n        \n        q2 = F.relu(self.l4(sa))\n        q2 = F.relu(self.l5(q2))\n        q2 = self.l6(q2)\n        \n        return q1, q2\n    \n    def Q1(self, state, action):\n        """Get first Q-value"""\n        sa = torch.cat([state, action], 1)\n        \n        q1 = F.relu(self.l1(sa))\n        q1 = F.relu(self.l2(q1))\n        q1 = self.l3(q1)\n        \n        return q1\n\n\nclass DDPGAgent:\n    """DDPG agent implementation for robotic control"""\n    \n    def __init__(self, state_dim, action_dim, max_action, lr=1e-4, tau=0.005, gamma=0.99):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.max_action = max_action\n        self.lr = lr\n        self.tau = tau  # Target network update rate\n        self.gamma = gamma  # Discount factor\n        \n        # Initialize networks\n        self.actor = Actor(state_dim, action_dim, max_action)\n        self.actor_target = Actor(state_dim, action_dim, max_action)\n        self.actor_target.load_state_dict(self.actor.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n        \n        self.critic = Critic(state_dim, action_dim)\n        self.critic_target = Critic(state_dim, action_dim)\n        self.critic_target.load_state_dict(self.critic.state_dict())\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n        \n        # Initialize replay buffer\n        self.replay_buffer = ReplayBuffer(1000000)\n        \n        # Noise for exploration\n        self.exploration_noise = 0.1\n        \n        # Device\n        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")\n        self.actor.to(self.device)\n        self.actor_target.to(self.device)\n        self.critic.to(self.device)\n        self.critic_target.to(self.device)\n    \n    def select_action(self, state, add_noise=True):\n        """Select action using the current policy"""\n        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n        action = self.actor(state).cpu().data.numpy().flatten()\n        \n        if add_noise:\n            noise = np.random.normal(0, self.exploration_noise, size=self.action_dim)\n            action = action + noise\n            action = np.clip(action, -self.max_action, self.max_action)\n        \n        return action\n    \n    def train(self, batch_size=256):\n        """Train the agent on a batch of experiences"""\n        if len(self.replay_buffer) < batch_size:\n            return  # Not enough experiences yet\n        \n        # Sample batch\n        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n        states = states.to(self.device)\n        actions = actions.to(self.device)\n        rewards = rewards.to(self.device)\n        next_states = next_states.to(self.device)\n        dones = dones.to(self.device)\n        \n        # Compute target Q-values\n        with torch.no_grad():\n            next_actions = self.actor_target(next_states)\n            target_Q1, target_Q2 = self.critic_target(next_states, next_actions)\n            target_Q = torch.min(target_Q1, target_Q2)\n            target_Q = rewards + (1 - dones) * self.gamma * target_Q\n        \n        # Update critic\n        current_Q1, current_Q2 = self.critic(states, actions)\n        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n        \n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n        \n        # Update actor\n        actor_actions = self.actor(states)\n        actor_loss = -self.critic.Q1(states, actor_actions).mean()\n        \n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n        \n        # Update target networks\n        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n        \n        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n        \n        return actor_loss.item(), critic_loss.item()\n    \n    def save(self, filename):\n        """Save the model"""\n        torch.save({\n            \'actor_state_dict\': self.actor.state_dict(),\n            \'actor_target_state_dict\': self.actor_target.state_dict(),\n            \'critic_state_dict\': self.critic.state_dict(),\n            \'critic_target_state_dict\': self.critic_target.state_dict(),\n            \'actor_optimizer_state_dict\': self.actor_optimizer.state_dict(),\n            \'critic_optimizer_state_dict\': self.critic_optimizer.state_dict()\n        }, f"{filename}_ddpg.pt")\n        print(f"Model saved to {filename}_ddpg.pt")\n    \n    def load(self, filename):\n        """Load the model"""\n        checkpoint = torch.load(f"{filename}_ddpg.pt", map_location=self.device)\n        self.actor.load_state_dict(checkpoint[\'actor_state_dict\'])\n        self.actor_target.load_state_dict(checkpoint[\'actor_target_state_dict\'])\n        self.critic.load_state_dict(checkpoint[\'critic_state_dict\'])\n        self.critic_target.load_state_dict(checkpoint[\'critic_target_state_dict\'])\n        self.actor_optimizer.load_state_dict(checkpoint[\'actor_optimizer_state_dict\'])\n        self.critic_optimizer.load_state_dict(checkpoint[\'critic_optimizer_state_dict\'])\n        print(f"Model loaded from {filename}_ddpg.pt")\n\n\nclass RobotEnvironment:\n    """Simulated robotic environment for training"""\n    \n    def __init__(self, task="reach"):\n        self.task = task\n        self.state_dim = 6  # [x, y, z, target_x, target_y, target_z]\n        self.action_dim = 3  # [dx, dy, dz] - relative position change\n        self.max_action = 0.1  # Maximum action magnitude\n        self.goal_threshold = 0.1  # Distance threshold to consider goal reached\n        \n        # Initialize state\n        self.reset()\n    \n    def reset(self):\n        """Reset the environment to initial state"""\n        # Random robot position\n        self.robot_pos = np.random.uniform(-1.0, 1.0, 3)\n        \n        # Random target position\n        self.target_pos = np.random.uniform(-1.0, 1.0, 3)\n        \n        # Ensure target is not too close initially\n        while np.linalg.norm(self.robot_pos - self.target_pos) < 0.5:\n            self.target_pos = np.random.uniform(-1.0, 1.0, 3)\n        \n        state = np.concatenate([self.robot_pos, self.target_pos])\n        return state\n    \n    def step(self, action):\n        """Execute action and return next state, reward, done, info"""\n        # Apply action (relative movement)\n        new_pos = self.robot_pos + action\n        \n        # Clamp position to workspace\n        new_pos = np.clip(new_pos, -2.0, 2.0)\n        \n        # Update robot position\n        self.robot_pos = new_pos\n        \n        # Calculate distance to target\n        dist = np.linalg.norm(self.robot_pos - self.target_pos)\n        \n        # Compute reward\n        reward = -dist  # Negative distance as reward\n        \n        # Add shaping reward for progress\n        prev_dist = np.linalg.norm(self.robot_pos - action - self.target_pos)\n        if dist < prev_dist:\n            reward += 0.1  # Small bonus for making progress\n        \n        # Add bonus for reaching target\n        if dist < self.goal_threshold:\n            reward += 10.0  # Large bonus for reaching target\n        \n        # Create new state\n        state = np.concatenate([self.robot_pos, self.target_pos])\n        \n        # Check if episode is done\n        done = dist < self.goal_threshold\n        if done:\n            # Reset target to continue training\n            self.target_pos = np.random.uniform(-1.0, 1.0, 3)\n            state = np.concatenate([self.robot_pos, self.target_pos])\n            # Don\'t set done=True in this case to continue training\n        \n        # Limit episode length\n        if not hasattr(self, \'step_count\'):\n            self.step_count = 0\n        self.step_count += 1\n        \n        if self.step_count > 1000:  # Max steps\n            done = True\n            self.step_count = 0\n        \n        info = {\'distance\': dist, \'reached_target\': dist < self.goal_threshold}\n        \n        return state, reward, done, info\n\n\ndef train_robot_rl_agent(episodes=1000, max_steps=1000):\n    """Train a robotic agent using DDPG"""\n    # Create environment\n    env = RobotEnvironment()\n    \n    # Create agent\n    agent = DDPGAgent(\n        state_dim=env.state_dim,\n        action_dim=env.action_dim,\n        max_action=env.max_action\n    )\n    \n    # Training loop\n    episode_rewards = []\n    \n    for episode in range(episodes):\n        state = env.reset()\n        episode_reward = 0\n        done = False\n        \n        for step in range(max_steps):\n            # Select action with noise\n            action = agent.select_action(state, add_noise=True)\n            \n            # Execute action\n            next_state, reward, done, info = env.step(action)\n            \n            # Store experience in replay buffer\n            agent.replay_buffer.push(state, action, reward, next_state, done)\n            \n            # Update state\n            state = next_state\n            episode_reward += reward\n            \n            # Train agent\n            if len(agent.replay_buffer) > 256:\n                actor_loss, critic_loss = agent.train()\n            \n            if done:\n                break\n        \n        episode_rewards.append(episode_reward)\n        \n        # Print progress\n        if episode % 100 == 0:\n            avg_reward = np.mean(episode_rewards[-100:])\n            print(f"Episode {episode}, Average Reward: {avg_reward:.2f}, Distance: {info.get(\'distance\', 0):.3f}")\n    \n    # Save trained agent\n    agent.save("robot_ddpg_agent")\n    \n    return agent, episode_rewards\n\n\ndef test_robot_agent(agent, episodes=10):\n    """Test the trained agent"""\n    env = RobotEnvironment()\n    \n    # Ensure agent is in eval mode\n    agent.actor.eval()\n    \n    success_count = 0\n    distances = []\n    \n    for episode in range(episodes):\n        state = env.reset()\n        episode_reward = 0\n        done = False\n        \n        print(f"Testing Episode {episode + 1}")\n        \n        for step in range(1000):  # Max steps for testing\n            # Select action without noise\n            action = agent.select_action(state, add_noise=False)\n            \n            # Execute action\n            state, reward, done, info = env.step(action)\n            episode_reward += reward\n            \n            distance = info[\'distance\']\n            distances.append(distance)\n            \n            if info.get(\'reached_target\', False):\n                success_count += 1\n                print(f"  Success! Reached target in {step} steps. Distance: {distance:.3f}")\n                break\n            \n            if done:\n                break\n        \n        print(f"  Episode reward: {episode_reward:.2f}")\n    \n    success_rate = success_count / episodes\n    avg_distance = np.mean(distances) if distances else float(\'inf\')\n    \n    print(f"\\nTesting Results:")\n    print(f"Success Rate: {success_rate:.2f} ({success_count}/{episodes})")\n    print(f"Average Distance: {avg_distance:.3f}")\n    \n    return success_rate, avg_distance\n\n\ndef main():\n    """Example usage of the RL for robotics"""\n    print("Reinforcement Learning for Robot Control")\n    print("=" * 50)\n    \n    print("Training DDPG agent for robotic control...")\n    agent, rewards = train_robot_rl_agent(episodes=1000)\n    \n    print(f"\\nTraining completed!")\n    print(f"Last episode reward: {rewards[-1] if rewards else 0:.2f}")\n    print(f"Average reward over last 100 episodes: {np.mean(rewards[-100:]) if len(rewards) >= 100 else np.mean(rewards):.2f}")\n    \n    print(f"\\nTesting trained agent...")\n    success_rate, avg_distance = test_robot_agent(agent, episodes=10)\n    \n    print("\\nThis example demonstrates:")\n    print("- Implementation of DDPG for continuous robotic control")\n    print("- Use of replay buffer for efficient learning")\n    print("- Actor-critic architecture for policy and value learning")\n    print("- Testing procedure to evaluate learned policies")\n    \n    print("\\nFor real robotic applications, additional considerations include:")\n    print("- Safety constraints and emergency stops")\n    print("- Transfer from simulation to physical robots")\n    print("- Integration with perception systems")\n    print("- Handling partial observability")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(n.h2,{id:"simulation-demonstration",children:"Simulation Demonstration"}),"\n",(0,r.jsx)(n.p,{children:"This implementation demonstrates how to apply reinforcement learning to robotic control tasks. The DDPG algorithm is used for continuous control, which is typical for robot applications. The example creates a simulated environment where a robot learns to reach target positions, which could be extended to more complex tasks like manipulation or navigation. The code can be integrated with real robot hardware after proper safety considerations."}),"\n",(0,r.jsx)(n.h2,{id:"hands-on-lab-reinforcement-learning-for-robot-control",children:"Hands-On Lab: Reinforcement Learning for Robot Control"}),"\n",(0,r.jsx)(n.p,{children:"In this lab, you'll implement and test RL algorithms for robotic control:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Implement the DDPG algorithm for continuous control"}),"\n",(0,r.jsx)(n.li,{children:"Create a simulated robotic environment"}),"\n",(0,r.jsx)(n.li,{children:"Train an agent to perform a robotic task"}),"\n",(0,r.jsx)(n.li,{children:"Transfer the learned policy to a physical robot (if available)"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate the performance and safety of the RL controller"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"required-equipment",children:"Required Equipment:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Python development environment"}),"\n",(0,r.jsx)(n.li,{children:"PyTorch or TensorFlow"}),"\n",(0,r.jsx)(n.li,{children:"(Optional) Robot simulation environment (Gazebo, Isaac Sim)"}),"\n",(0,r.jsx)(n.li,{children:"(Optional) Physical robot for testing"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"instructions",children:"Instructions:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create a new Python package for RL experiments"}),"\n",(0,r.jsx)(n.li,{children:"Implement the DDPG agent as shown in the example"}),"\n",(0,r.jsx)(n.li,{children:"Create a robotic task environment (e.g., reaching, navigation)"}),"\n",(0,r.jsx)(n.li,{children:"Train the agent on the task with appropriate reward shaping"}),"\n",(0,r.jsx)(n.li,{children:"Test the learned policy in simulation"}),"\n",(0,r.jsx)(n.li,{children:"(If available) Test on a physical robot with safety measures"}),"\n",(0,r.jsx)(n.li,{children:"Evaluate the performance metrics (success rate, efficiency)"}),"\n",(0,r.jsx)(n.li,{children:"Document the training process and results"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"common-pitfalls--debugging-notes",children:"Common Pitfalls & Debugging Notes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sample Efficiency"}),": RL algorithms often require many samples; use simulation for pre-training"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reward Shaping"}),": Poor reward functions can lead to unexpected behaviors"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety Concerns"}),": Physical robots need safety constraints during learning"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Hyperparameter Sensitivity"}),": RL algorithms can be sensitive to hyperparameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exploration vs. Safety"}),": Balancing exploration with safety requirements"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transfer Issues"}),": Policies learned in simulation may not transfer to reality"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational Requirements"}),": RL training can be computationally intensive"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"summary--key-terms",children:"Summary & Key Terms"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Key Terms:"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Markov Decision Process (MDP)"}),": Mathematical framework for RL problems"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Actor-Critic"}),": Architecture with separate policy and value networks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Experience Replay"}),": Technique to break correlation in training data"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Target Networks"}),": Slowly updated networks for stable training"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Policy Gradient"}),": Method for optimizing policy parameters"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Continuous Control"}),": RL with continuous action spaces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sample Efficiency"}),": How many samples needed to learn effective policy"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"further-reading--citations",children:"Further Reading & Citations"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:'Lillicrap, T. P., et al. (2015). "Continuous control with deep reinforcement learning." arXiv preprint arXiv:1509.02971.'}),"\n",(0,r.jsx)(n.li,{children:'Schulman, J., et al. (2015). "Trust region policy optimization." International Conference on Machine Learning.'}),"\n",(0,r.jsx)(n.li,{children:'Levine, S., et al. (2016). "End-to-end training of deep visuomotor policies." Journal of Machine Learning Research.'}),"\n",(0,r.jsx)(n.li,{children:'Pomerleau, D. A. (1989). "ALVINN: An autonomous land vehicle in a neural network." Advances in Neural Information Processing Systems.'}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Explain how the DDPG algorithm addresses the challenges of continuous control in robotics."}),"\n",(0,r.jsx)(n.li,{children:"What are the key components of a reinforcement learning system for robot control?"}),"\n",(0,r.jsx)(n.li,{children:"Describe the role of experience replay in improving sample efficiency."}),"\n",(0,r.jsx)(n.li,{children:"How would you design a reward function for a robotic manipulation task?"}),"\n",(0,r.jsx)(n.li,{children:"What safety considerations are essential when applying RL to physical robots?"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous"}),": ",(0,r.jsx)(n.a,{href:"/docs/appendices/hardware",children:"Hardware Requirements and Lab Options"}),(0,r.jsx)(n.br,{}),"\n",(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"/docs/appendices/sim-to-real",children:"Appendix C: Sim-to-Real Transfer Techniques"})]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);