"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[256],{7674:i=>{i.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"category","label":"Getting Started","items":[{"type":"link","href":"/docs/intro","label":"Introduction","docId":"intro","unlisted":false},{"type":"link","href":"/docs/prerequisites","label":"Prerequisites","docId":"prerequisites","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Physical AI Fundamentals","items":[{"type":"link","href":"/docs/physical-ai/intro","label":"Introduction to Physical AI & Embodied Intelligence","docId":"physical-ai/intro","unlisted":false},{"type":"link","href":"/docs/physical-ai/sensors-perception","label":"Sensors and Physical Perception","docId":"physical-ai/sensors-perception","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"ROS 2 Basics","items":[{"type":"link","href":"/docs/ros2-basics/intro","label":"Introduction to ROS 2 - The Robotic Nervous System","docId":"ros2-basics/intro","unlisted":false},{"type":"link","href":"/docs/ros2-basics/nodes-topics","label":"ROS 2 Nodes, Topics, Services, and Actions","docId":"ros2-basics/nodes-topics","unlisted":false},{"type":"link","href":"/docs/ros2-basics/urdf","label":"URDF Modeling for Robotics","docId":"ros2-basics/urdf","unlisted":false},{"type":"link","href":"/docs/ros2-basics/rclpy","label":"rclpy Python Client Library","docId":"ros2-basics/rclpy","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Simulation","items":[{"type":"link","href":"/docs/simulation/intro","label":"Introduction to Simulation Pipelines","docId":"simulation/intro","unlisted":false},{"type":"link","href":"/docs/simulation/gazebo","label":"Gazebo Physics Simulation","docId":"simulation/gazebo","unlisted":false},{"type":"link","href":"/docs/simulation/digital-twins","label":"Digital Twins and Sensor Simulation","docId":"simulation/digital-twins","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"NVIDIA Isaac Platform","items":[{"type":"link","href":"/docs/isaac-platform/intro","label":"Introduction to NVIDIA Isaac Platform","docId":"isaac-platform/intro","unlisted":false},{"type":"link","href":"/docs/isaac-platform/isaac-sim","label":"Isaac Sim Photorealistic Simulation","docId":"isaac-platform/isaac-sim","unlisted":false},{"type":"link","href":"/docs/isaac-platform/isaac-ros","label":"Isaac ROS Acceleration","docId":"isaac-platform/isaac-ros","unlisted":false},{"type":"link","href":"/docs/isaac-platform/vslam-nav2","label":"VSLAM and Navigation with Isaac ROS","docId":"isaac-platform/vslam-nav2","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Humanoid Robotics","items":[{"type":"link","href":"/docs/humanoid-robotics/intro","label":"Introduction to Humanoid Robotics","docId":"humanoid-robotics/intro","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/kinematics","label":"Humanoid Kinematics and Dynamics","docId":"humanoid-robotics/kinematics","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/locomotion","label":"Bipedal Locomotion and Walking Control","docId":"humanoid-robotics/locomotion","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/manipulation","label":"Manipulation & Humanoid Hands","docId":"humanoid-robotics/manipulation","unlisted":false},{"type":"link","href":"/docs/humanoid-robotics/hri","label":"Human-Robot Interaction (HRI)","docId":"humanoid-robotics/hri","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Vision-Language-Action Robotics","items":[{"type":"link","href":"/docs/vla-robotics/intro","label":"Vision-Language-Action Robotics","docId":"vla-robotics/intro","unlisted":false},{"type":"link","href":"/docs/vla-robotics/whisper","label":"Whisper for Voice Recognition","docId":"vla-robotics/whisper","unlisted":false},{"type":"link","href":"/docs/vla-robotics/llm-planning","label":"LLM-Based Planning for Robotics","docId":"vla-robotics/llm-planning","unlisted":false},{"type":"link","href":"/docs/vla-robotics/action-execution","label":"Action Execution Pipeline","docId":"vla-robotics/action-execution","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Capstone Project","items":[{"type":"link","href":"/docs/capstone/intro","label":"Capstone Project: Autonomous Humanoid Robot","docId":"capstone/intro","unlisted":false},{"type":"link","href":"/docs/capstone/system-design","label":"System Design and Implementation","docId":"capstone/system-design","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Appendices","items":[{"type":"link","href":"/docs/appendices/hardware","label":"Appendix A: Hardware Requirements and Lab Options","docId":"appendices/hardware","unlisted":false},{"type":"link","href":"/docs/appendices/reinforcement-learning","label":"Appendix B: Reinforcement Learning for Robot Control","docId":"appendices/reinforcement-learning","unlisted":false},{"type":"link","href":"/docs/appendices/sim-to-real","label":"Appendix C: Sim-to-Real Transfer Techniques","docId":"appendices/sim-to-real","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"appendices/hardware":{"id":"appendices/hardware","title":"Appendix A: Hardware Requirements and Lab Options","description":"This appendix provides detailed hardware requirements for implementing the Physical AI & Humanoid Robotics systems described in this textbook. The requirements are organized by development phase and system complexity, with options for different budget constraints and application scenarios.","sidebar":"tutorialSidebar"},"appendices/reinforcement-learning":{"id":"appendices/reinforcement-learning","title":"Appendix B: Reinforcement Learning for Robot Control","description":"Reinforcement Learning (RL) has emerged as a powerful paradigm for robot control, enabling robots to learn complex behaviors through interaction with their environment. This appendix covers the fundamentals of RL for robotics, implementation techniques, simulation-to-reality transfer, and practical applications. RL is particularly valuable for robotics because it can handle high-dimensional continuous state and action spaces, and can learn behaviors that are difficult to program explicitly.","sidebar":"tutorialSidebar"},"appendices/sim-to-real":{"id":"appendices/sim-to-real","title":"Appendix C: Sim-to-Real Transfer Techniques","description":"Sim-to-real transfer is a critical capability in robotics that enables policies trained in simulation to operate effectively on physical robots. This approach significantly reduces the time and resources needed for robot learning by leveraging the safety and speed of simulation while addressing the reality gap between virtual and real environments. This appendix covers techniques, challenges, and best practices for successful sim-to-real transfer.","sidebar":"tutorialSidebar"},"capstone/intro":{"id":"capstone/intro","title":"Capstone Project: Autonomous Humanoid Robot","description":"The capstone project represents the culmination of all concepts covered in this textbook, bringing together Physical AI, ROS 2, simulation, NVIDIA Isaac Platform, Vision-Language-Action capabilities, and humanoid robotics fundamentals. Students will design and implement an autonomous humanoid robot system capable of understanding voice commands, navigating environments, detecting and manipulating objects, and executing complex tasks in a human-like manner.","sidebar":"tutorialSidebar"},"capstone/system-design":{"id":"capstone/system-design","title":"System Design and Implementation","description":"The system design of our autonomous humanoid robot represents the culmination of all concepts covered in this textbook. This chapter details the architectural decisions, implementation strategies, and engineering considerations that went into creating a complete, functional robotic system. The design follows modern robotics principles while integrating Physical AI, ROS 2, simulation, NVIDIA Isaac, Vision-Language-Action capabilities, and humanoid robotics fundamentals.","sidebar":"tutorialSidebar"},"humanoid-robotics/hri":{"id":"humanoid-robotics/hri","title":"Human-Robot Interaction (HRI)","description":"Human-Robot Interaction (HRI) is a critical aspect of humanoid robotics that focuses on the design, evaluation, and implementation of robots that interact with humans. As humanoid robots become more prevalent in our daily lives, creating natural, intuitive, and safe interaction methods becomes increasingly important. HRI encompasses multiple modalities including verbal communication, gesture interpretation, facial expressions, and collaborative task execution.","sidebar":"tutorialSidebar"},"humanoid-robotics/intro":{"id":"humanoid-robotics/intro","title":"Introduction to Humanoid Robotics","description":"Humanoid robotics represents one of the most ambitious fields in robotics, focusing on the design, development, and control of robots that physically resemble humans. These robots are characterized by their bipedal locomotion, dexterous manipulation capabilities, and often anthropomorphic form factor. Humanoid robots have the potential to operate in human environments, interact naturally with people, and perform tasks designed for humans.","sidebar":"tutorialSidebar"},"humanoid-robotics/kinematics":{"id":"humanoid-robotics/kinematics","title":"Humanoid Kinematics and Dynamics","description":"Humanoid kinematics and dynamics form the mathematical foundation for understanding and controlling the motion of humanoid robots. Kinematics deals with the geometric relationships between joints and end-effectors without considering forces, while dynamics includes the effects of forces, torques, masses, and accelerations. This section covers both forward and inverse kinematics as well as dynamic modeling approaches for humanoid robots.","sidebar":"tutorialSidebar"},"humanoid-robotics/locomotion":{"id":"humanoid-robotics/locomotion","title":"Bipedal Locomotion and Walking Control","description":"Bipedal locomotion is one of the most challenging aspects of humanoid robotics, requiring precise control of multiple degrees of freedom while maintaining dynamic balance. Unlike wheeled or tracked robots, humanoid robots must manage their center of mass to prevent falling while walking, running, or performing other dynamic movements. This section explores the principles, algorithms, and control strategies for achieving stable bipedal locomotion.","sidebar":"tutorialSidebar"},"humanoid-robotics/manipulation":{"id":"humanoid-robotics/manipulation","title":"Manipulation & Humanoid Hands","description":"Manipulation is a fundamental capability that gives humanoid robots the ability to interact with their environment by grasping, manipulating, and releasing objects. This section explores the specialized challenges of dexterous manipulation for humanoid robots, including the biomechanics of human-like hands, grasp planning, and control strategies for performing complex manipulation tasks.","sidebar":"tutorialSidebar"},"index":{"id":"index","title":"Physical AI & Humanoid Robotics Textbook","description":"This comprehensive textbook teaches students how to build AI-driven humanoid robots using ROS 2, Gazebo, Unity, and NVIDIA Isaac, culminating in a full Vision-Language-Action humanoid capstone project."},"intro":{"id":"intro","title":"Introduction","description":"Welcome to the Physical AI & Humanoid Robotics textbook. This comprehensive resource will guide you through the fundamental concepts and practical implementations required to build AI-driven humanoid robots.","sidebar":"tutorialSidebar"},"isaac-platform/intro":{"id":"isaac-platform/intro","title":"Introduction to NVIDIA Isaac Platform","description":"The NVIDIA Isaac Platform is a comprehensive solution for developing, simulating, and deploying robotics applications that leverage the power of AI and accelerated computing. It consists of Isaac Sim for photorealistic simulation, Isaac ROS for accelerated perception and navigation algorithms, and Isaac Lab for reinforcement learning. The platform is designed to bridge the gap between simulation and reality, enabling the development of robust perception, navigation, and manipulation capabilities.","sidebar":"tutorialSidebar"},"isaac-platform/isaac-ros":{"id":"isaac-platform/isaac-ros","title":"Isaac ROS Acceleration","description":"Isaac ROS is a collection of hardware-accelerated perception and navigation packages for ROS 2. It leverages NVIDIA\'s GPU computing capabilities to accelerate computationally intensive robotics tasks such as sensor processing, perception, and navigation. This acceleration is critical for achieving real-time performance in complex robotic applications.","sidebar":"tutorialSidebar"},"isaac-platform/isaac-sim":{"id":"isaac-platform/isaac-sim","title":"Isaac Sim Photorealistic Simulation","description":"Isaac Sim is NVIDIA\'s robotics simulation application built on the Omniverse platform, designed to provide photorealistic rendering and accurate physics simulation for robotics applications. It enables the development and testing of complex robotic systems in virtual environments that closely match real-world conditions, significantly reducing development time and cost while improving safety.","sidebar":"tutorialSidebar"},"isaac-platform/vslam-nav2":{"id":"isaac-platform/vslam-nav2","title":"VSLAM and Navigation with Isaac ROS","description":"Visual Simultaneous Localization and Mapping (VSLAM) and navigation are critical capabilities for autonomous robots. The NVIDIA Isaac Platform provides accelerated implementations of these algorithms through Isaac ROS, leveraging GPU hardware to achieve real-time performance. This section explores how to implement and optimize VSLAM and navigation systems using Isaac ROS packages.","sidebar":"tutorialSidebar"},"meta/glossary":{"id":"meta/glossary","title":"Glossary of Robotics Terms","description":"This glossary provides definitions for key terms used throughout the Physical AI & Humanoid Robotics textbook. Terms are organized alphabetically for easy reference."},"physical-ai/intro":{"id":"physical-ai/intro","title":"Introduction to Physical AI & Embodied Intelligence","description":"Physical AI represents a paradigm shift from traditional artificial intelligence to intelligence that exists and operates in the physical world. Unlike conventional AI that processes data in virtual environments, Physical AI agents must perceive, reason, and act in real-world environments with all their complexity, uncertainty, and dynamic nature.","sidebar":"tutorialSidebar"},"physical-ai/sensors-perception":{"id":"physical-ai/sensors-perception","title":"Sensors and Physical Perception","description":"In Physical AI systems, sensors serve as the primary interface between the robot and its environment. The ability to accurately perceive the physical world is fundamental to making informed decisions and executing appropriate actions. This section will explore the different types of sensors used in robotics and how they enable physical perception.","sidebar":"tutorialSidebar"},"prerequisites":{"id":"prerequisites","title":"Prerequisites","description":"Before diving into the Physical AI & Humanoid Robotics textbook, you need to set up your development environment with the required software and tools.","sidebar":"tutorialSidebar"},"ros2-basics/intro":{"id":"ros2-basics/intro","title":"Introduction to ROS 2 - The Robotic Nervous System","description":"The Robot Operating System 2 (ROS 2) serves as the nervous system for robotic applications, providing a framework for writing robot software. Unlike a traditional operating system, ROS 2 is a collection of tools, libraries, and conventions that aim to simplify the development of complex robotic applications by providing hardware abstraction, device drivers, libraries, visualizers, message-passing, package management, and more.","sidebar":"tutorialSidebar"},"ros2-basics/nodes-topics":{"id":"ros2-basics/nodes-topics","title":"ROS 2 Nodes, Topics, Services, and Actions","description":"This section provides a deeper dive into the core communication patterns in ROS 2: nodes, topics, services, and actions. Understanding these concepts is essential for building modular, distributed robotic applications.","sidebar":"tutorialSidebar"},"ros2-basics/rclpy":{"id":"ros2-basics/rclpy","title":"rclpy Python Client Library","description":"The Robot Client Library for Python (rclpy) provides the Python API for ROS 2. It allows Python developers to create ROS 2 nodes, publish and subscribe to topics, make service calls, and interact with actions. Understanding rclpy is crucial for developing robotic applications in Python within the ROS 2 ecosystem.","sidebar":"tutorialSidebar"},"ros2-basics/urdf":{"id":"ros2-basics/urdf","title":"URDF Modeling for Robotics","description":"The Unified Robot Description Format (URDF) is an XML-based format used in ROS to describe robot models. This includes aspects of the robot such as kinematic and dynamic descriptions, visual and collision properties, and sensor mounting positions. Understanding URDF is essential for simulating robots, planning motions, and controlling robot hardware.","sidebar":"tutorialSidebar"},"simulation/digital-twins":{"id":"simulation/digital-twins","title":"Digital Twins and Sensor Simulation","description":"Digital twins in robotics represent virtual replicas of physical systems that mirror their real-world counterparts\' behaviors, characteristics, and states in real-time. Combined with accurate sensor simulation, digital twins provide an invaluable platform for testing, validating, and training robotic systems before deployment in the physical world.","sidebar":"tutorialSidebar"},"simulation/gazebo":{"id":"simulation/gazebo","title":"Gazebo Physics Simulation","description":"Gazebo is a 3D dynamic simulator designed specifically for robotics applications. It provides accurate physics simulation, realistic rendering, and convenient interfaces for both the Robot Operating System (ROS) and standalone applications. Gazebo enables roboticists to test algorithms, train AI systems, and perform regression testing without the need for physical hardware.","sidebar":"tutorialSidebar"},"simulation/intro":{"id":"simulation/intro","title":"Introduction to Simulation Pipelines","description":"Simulation is a critical component in robotics development, providing a safe, cost-effective, and efficient environment for testing and validating robotic systems. In the context of Physical AI, simulation enables researchers and developers to experiment with complex behaviors, test algorithms, and iterate on designs without the constraints and risks associated with physical hardware. This chapter explores the key simulation environments used in robotics: Gazebo for physics simulation, Unity for visualization, and digital twin methodologies.","sidebar":"tutorialSidebar"},"vla-robotics/action-execution":{"id":"vla-robotics/action-execution","title":"Action Execution Pipeline","description":"The action execution pipeline is the critical component that connects high-level plans generated by language models and reasoning systems to low-level robot controls. This pipeline ensures that abstract commands are translated into precise robotic actions while maintaining safety, handling failures, and adapting to environmental changes. The execution pipeline must be robust, responsive, and capable of real-time operation.","sidebar":"tutorialSidebar"},"vla-robotics/intro":{"id":"vla-robotics/intro","title":"Vision-Language-Action Robotics","description":"Vision-Language-Action (VLA) robotics represents a paradigm shift in how robots understand and interact with the world. Rather than treating perception, language understanding, and action execution as separate modules, VLA systems integrate these capabilities into unified architectures that can process natural language commands, perceive complex environments, and execute sophisticated manipulation tasks. This integration enables more natural human-robot interaction and more flexible robotic behaviors.","sidebar":"tutorialSidebar"},"vla-robotics/llm-planning":{"id":"vla-robotics/llm-planning","title":"LLM-Based Planning for Robotics","description":"Large Language Models (LLMs) have emerged as powerful tools for robotics, particularly for high-level planning, natural language understanding, and generating executable robotic actions from natural language commands. LLMs can interpret complex instructions, reason about the environment, break down tasks into sub-goals, and generate pseudo-code or action sequences for robotic execution. This section explores how LLMs can be integrated into robotic planning systems.","sidebar":"tutorialSidebar"},"vla-robotics/whisper":{"id":"vla-robotics/whisper","title":"Whisper for Voice Recognition","description":"Whisper is a state-of-the-art automatic speech recognition (ASR) system developed by OpenAI that plays a crucial role in Vision-Language-Action (VLA) robotic systems. It enables robots to understand natural language commands through speech, providing an intuitive and human-like interaction modality. Whisper\'s robust performance across multiple languages and its ability to handle varying audio conditions make it valuable for robotics applications.","sidebar":"tutorialSidebar"}}}}')}}]);