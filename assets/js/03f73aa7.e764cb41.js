"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[433],{203:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"appendices/sim-to-real","title":"Appendix C: Sim-to-Real Transfer Techniques","description":"Sim-to-real transfer is a critical capability in robotics that enables policies trained in simulation to operate effectively on physical robots. This approach significantly reduces the time and resources needed for robot learning by leveraging the safety and speed of simulation while addressing the reality gap between virtual and real environments. This appendix covers techniques, challenges, and best practices for successful sim-to-real transfer.","source":"@site/docs/08-appendices/sim-to-real.md","sourceDirName":"08-appendices","slug":"/appendices/sim-to-real","permalink":"/docs/appendices/sim-to-real","draft":false,"unlisted":false,"editUrl":"https://github.com/shaheryarshah/Hackthon_SpecKitPlus/edit/main/docs/docs/08-appendices/sim-to-real.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Appendix B: Reinforcement Learning for Robot Control","permalink":"/docs/appendices/reinforcement-learning"}}');var t=i(4848),r=i(8453);const o={},s="Appendix C: Sim-to-Real Transfer Techniques",l={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Core Concepts",id:"core-concepts",level:2},{value:"Reality Gap",id:"reality-gap",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"System Identification",id:"system-identification",level:3},{value:"Robust Control",id:"robust-control",level:3},{value:"Equations and Models",id:"equations-and-models",level:2},{value:"Domain Randomization Model",id:"domain-randomization-model",level:3},{value:"Transfer Success Metric",id:"transfer-success-metric",level:3},{value:"Domain Adaptation Loss",id:"domain-adaptation-loss",level:3},{value:"System Identification Model",id:"system-identification-model",level:3},{value:"Code Example: Domain Randomization for Sim-to-Real Transfer",id:"code-example-domain-randomization-for-sim-to-real-transfer",level:2},{value:"Simulation Demonstration",id:"simulation-demonstration",level:2},{value:"Hands-On Lab: Sim-to-Real Transfer Implementation",id:"hands-on-lab-sim-to-real-transfer-implementation",level:2},{value:"Required Equipment:",id:"required-equipment",level:3},{value:"Instructions:",id:"instructions",level:3},{value:"Common Pitfalls &amp; Debugging Notes",id:"common-pitfalls--debugging-notes",level:2},{value:"Summary &amp; Key Terms",id:"summary--key-terms",level:2},{value:"Further Reading &amp; Citations",id:"further-reading--citations",level:2},{value:"Assessment Questions",id:"assessment-questions",level:2}];function c(n){const e={a:"a",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"appendix-c-sim-to-real-transfer-techniques",children:"Appendix C: Sim-to-Real Transfer Techniques"})}),"\n",(0,t.jsx)(e.p,{children:"Sim-to-real transfer is a critical capability in robotics that enables policies trained in simulation to operate effectively on physical robots. This approach significantly reduces the time and resources needed for robot learning by leveraging the safety and speed of simulation while addressing the reality gap between virtual and real environments. This appendix covers techniques, challenges, and best practices for successful sim-to-real transfer."}),"\n",(0,t.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(e.p,{children:"After studying this appendix, you should be able to:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Understand the challenges and causes of the sim-to-real transfer problem"}),"\n",(0,t.jsx)(e.li,{children:"Apply domain randomization techniques to improve transferability"}),"\n",(0,t.jsx)(e.li,{children:"Implement system identification and system matching methods"}),"\n",(0,t.jsx)(e.li,{children:"Design robust control policies that are resilient to modeling errors"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate the success of sim-to-real transfer approaches"}),"\n",(0,t.jsx)(e.li,{children:"Select appropriate sim-to-real techniques for different robotic applications"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"core-concepts",children:"Core Concepts"}),"\n",(0,t.jsx)(e.h3,{id:"reality-gap",children:"Reality Gap"}),"\n",(0,t.jsx)(e.p,{children:"The reality gap refers to the differences between simulated and real robot behavior:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visual Differences"}),": Lighting, textures, camera noise, resolution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Physical Differences"}),": Friction, compliance, dynamics, sensor noise"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Modeling Inaccuracies"}),": Approximations in physical simulation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Temporal Differences"}),": Timing, latency, and synchronization issues"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,t.jsx)(e.p,{children:"A technique that improves generalization by randomizing simulation parameters:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Visual Randomization"}),": Lighting, textures, colors, camera parameters"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Physical Randomization"}),": Masses, frictions, inertias, dynamics parameters"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sensor Randomization"}),": Noise models, delays, inaccuracies"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Environmental Randomization"}),": Object properties, surface conditions"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"system-identification",children:"System Identification"}),"\n",(0,t.jsx)(e.p,{children:"The process of finding accurate models of real robot dynamics:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Parameter Estimation"}),": Finding accurate physical parameters"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Model Learning"}),": Learning complete dynamics models"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Friction Modeling"}),": Accurate modeling of friction effects"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Actuator Characterization"}),": Modeling actuator dynamics and delays"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"robust-control",children:"Robust Control"}),"\n",(0,t.jsx)(e.p,{children:"Control strategies that maintain performance despite model errors:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robust Policies"}),": Policies that handle uncertainty and disturbances"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Adaptive Control"}),": Controllers that adjust to changing conditions"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Model Predictive Control"}),": Controllers that account for model uncertainty"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Impedance Control"}),": Compliance-based control strategies"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"equations-and-models",children:"Equations and Models"}),"\n",(0,t.jsx)(e.h3,{id:"domain-randomization-model",children:"Domain Randomization Model"}),"\n",(0,t.jsx)(e.p,{children:"The domain randomization approach can be formalized as:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u03c0* = argmax_\u03c0 E_{s~P_random}[\u2211_t \u03b3^t R(s_t, \u03c0(s_t))]\n"})}),"\n",(0,t.jsx)(e.p,{children:"Where:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03c0*"})," is the optimal policy"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"P_random"})," is the randomized simulation dynamics"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03b3"})," is the discount factor"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"R"})," is the reward function"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"transfer-success-metric",children:"Transfer Success Metric"}),"\n",(0,t.jsx)(e.p,{children:"The success of transfer can be measured by:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"T = (P_real(\u03c0_sim) - P_random(\u03c0_sim)) / (P_real(\u03c0_real) - P_random(\u03c0_real))\n"})}),"\n",(0,t.jsx)(e.p,{children:"Where:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"T"})," is the transfer quality (0 to 1, where 1 is perfect transfer)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"P_real"})," and ",(0,t.jsx)(e.code,{children:"P_random"})," are performance in real and random domains"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03c0_sim"})," is policy trained in randomized simulation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03c0_real"})," is policy trained in real environment"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"domain-adaptation-loss",children:"Domain Adaptation Loss"}),"\n",(0,t.jsx)(e.p,{children:"The loss function for domain adaptation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"L = L_task + \u03bb * L_domain\n"})}),"\n",(0,t.jsx)(e.p,{children:"Where:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"L_task"})," is the task-specific loss (e.g., policy loss)"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"L_domain"})," is the domain discrepancy loss"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03bb"})," is the trade-off parameter"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"system-identification-model",children:"System Identification Model"}),"\n",(0,t.jsx)(e.p,{children:"The process of identifying system parameters:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"\u03b8* = argmin_\u03b8 ||y_real - f_\u03b8(x_sim)||\n"})}),"\n",(0,t.jsx)(e.p,{children:"Where:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"\u03b8*"})," are the identified parameters"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"y_real"})," is real system response"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"f_\u03b8"})," is the simulation model parameterized by \u03b8"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.code,{children:"x_sim"})," is the simulation input"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"code-example-domain-randomization-for-sim-to-real-transfer",children:"Code Example: Domain Randomization for Sim-to-Real Transfer"}),"\n",(0,t.jsx)(e.p,{children:"Here's an implementation of domain randomization techniques:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nfrom abc import ABC, abstractmethod\nimport gym\nfrom gym import spaces\n\n\nclass DomainRandomizationWrapper(gym.Wrapper):\n    """\n    Environment wrapper that applies domain randomization\n    """\n    def __init__(self, env, randomization_config):\n        super().__init__(env)\n        self.randomization_config = randomization_config\n        self.current_params = {}\n        \n        # Initialize randomization ranges\n        self.param_ranges = {\n            \'mass_range\': randomization_config.get(\'mass_range\', [0.8, 1.2]),\n            \'friction_range\': randomization_config.get(\'friction_range\', [0.5, 1.5]),\n            \'com_range\': randomization_config.get(\'com_range\', [-0.1, 0.1]),\n            \'actuator_range\': randomization_config.get(\'actuator_range\', [0.9, 1.1]),\n            \'sensor_noise_range\': randomization_config.get(\'sensor_noise_range\', [0.0, 0.05])\n        }\n        \n        # Apply initial randomization\n        self.randomize_domain()\n    \n    def randomize_domain(self):\n        """\n        Randomize physical parameters in the environment\n        """\n        # Randomize mass parameters\n        if \'mass_range\' in self.param_ranges:\n            mass_multiplier = np.random.uniform(*self.param_ranges[\'mass_range\'])\n            self.current_params[\'mass_multiplier\'] = mass_multiplier\n            # In a real environment, this would update the simulation\n            # self.env.set_mass_multiplier(mass_multiplier)\n        \n        # Randomize friction parameters\n        if \'friction_range\' in self.param_ranges:\n            friction_multiplier = np.random.uniform(*self.param_ranges[\'friction_range\'])\n            self.current_params[\'friction_multiplier\'] = friction_multiplier\n            # self.env.set_friction_multiplier(friction_multiplier)\n        \n        # Randomize center of mass\n        if \'com_range\' in self.param_ranges:\n            com_offset = np.random.uniform(*self.param_ranges[\'com_range\'], size=3)\n            self.current_params[\'com_offset\'] = com_offset\n            # self.env.set_com_offset(com_offset)\n        \n        # Randomize actuator parameters\n        if \'actuator_range\' in self.param_ranges:\n            actuator_multiplier = np.random.uniform(*self.param_ranges[\'actuator_range\'])\n            self.current_params[\'actuator_multiplier\'] = actuator_multiplier\n            # self.env.set_actuator_multiplier(actuator_multiplier)\n        \n        # Update environment with new parameters\n        self.apply_randomization()\n    \n    def apply_randomization(self):\n        """\n        Apply the randomized parameters to the environment\n        """\n        # In a real implementation, this would call environment-specific functions\n        # to update the simulation with randomized parameters\n        pass\n    \n    def step(self, action):\n        """\n        Execute step with potential sensor noise\n        """\n        observation, reward, done, info = self.env.step(action)\n        \n        # Add sensor noise based on randomization\n        if \'sensor_noise_range\' in self.param_ranges:\n            noise_level = np.random.uniform(*self.param_ranges[\'sensor_noise_range\'])\n            noise = np.random.normal(0, noise_level, size=observation.shape)\n            observation = observation + noise\n        \n        info[\'randomized_params\'] = self.current_params\n        return observation, reward, done, info\n    \n    def reset(self, **kwargs):\n        """\n        Reset environment with new randomization\n        """\n        # Apply new randomization before reset\n        self.randomize_domain()\n        return self.env.reset(**kwargs)\n\n\nclass DynamicsRandomizer:\n    """\n    Class for randomizing robot dynamics parameters\n    """\n    def __init__(self, initial_params, randomization_config):\n        self.initial_params = initial_params\n        self.randomization_config = randomization_config\n        self.current_params = initial_params.copy()\n    \n    def randomize(self):\n        """\n        Randomize dynamics parameters according to configuration\n        """\n        randomized_params = self.initial_params.copy()\n        \n        # Randomize masses\n        if \'mass_randomization\' in self.randomization_config:\n            config = self.randomization_config[\'mass_randomization\']\n            for link_name in config[\'links\']:\n                if link_name in randomized_params.get(\'masses\', {}):\n                    multiplier = np.random.uniform(\n                        config[\'range\'][0], \n                        config[\'range\'][1]\n                    )\n                    randomized_params[\'masses\'][link_name] *= multiplier\n        \n        # Randomize friction\n        if \'friction_randomization\' in self.randomization_config:\n            config = self.randomization_config[\'friction_randomization\']\n            for joint_name in config[\'joints\']:\n                if joint_name in randomized_params.get(\'friction\', {}):\n                    multiplier = np.random.uniform(\n                        config[\'range\'][0], \n                        config[\'range\'][1]\n                    )\n                    randomized_params[\'friction\'][joint_name] *= multiplier\n        \n        # Randomize gear ratios\n        if \'gear_randomization\' in self.randomization_config:\n            config = self.randomization_config[\'gear_randomization\']\n            for joint_name in config[\'joints\']:\n                if joint_name in randomized_params.get(\'gears\', {}):\n                    multiplier = np.random.uniform(\n                        config[\'range\'][0], \n                        config[\'range\'][1]\n                    )\n                    randomized_params[\'gears\'][joint_name] *= multiplier\n        \n        # Randomize actuator dynamics\n        if \'actuator_randomization\' in self.randomization_config:\n            config = self.randomization_config[\'actuator_randomization\']\n            for joint_name in config[\'joints\']:\n                if joint_name in randomized_params.get(\'actuators\', {}):\n                    # Randomize delay, bandwidth, etc.\n                    delay_range = config.get(\'delay_range\', [0.0, 0.02])\n                    delay = np.random.uniform(*delay_range)\n                    randomized_params[\'actuators\'][joint_name][\'delay\'] = delay\n                    \n                    bandwidth_range = config.get(\'bandwidth_range\', [0.8, 1.2])\n                    bandwidth = np.random.uniform(*bandwidth_range)\n                    randomized_params[\'actuators\'][joint_name][\'bandwidth\'] = bandwidth\n        \n        self.current_params = randomized_params\n        return randomized_params\n    \n    def get_current_params(self):\n        """\n        Get current randomized parameters\n        """\n        return self.current_params\n\n\nclass SystemID:\n    """\n    System identification for robot dynamics\n    """\n    def __init__(self, robot_model):\n        self.robot_model = robot_model\n        self.excitation_data = []\n        self.param_history = []\n    \n    def collect_excitation_data(self, action_sequence, dt=0.01, duration=10.0):\n        """\n        Collect data for system identification by exciting the system\n        """\n        print("Collecting excitation data for system identification...")\n        \n        # Reset robot to initial state\n        # self.robot_model.reset()\n        \n        # Apply random input sequence\n        num_steps = int(duration / dt)\n        for i in range(num_steps):\n            # Generate random action\n            action = np.random.uniform(\n                low=self.robot_model.action_space.low,\n                high=self.robot_model.action_space.high\n            )\n            \n            # Get robot response\n            state = self.robot_model.get_state()\n            next_state, reward, done, info = self.robot_model.step(action)\n            \n            # Store data\n            self.excitation_data.append({\n                \'time\': i * dt,\n                \'action\': action,\n                \'state\': state,\n                \'next_state\': next_state,\n                \'reward\': reward\n            })\n        \n        print(f"Collected {len(self.excitation_data)} data points")\n    \n    def estimate_parameters(self, method=\'least_squares\'):\n        """\n        Estimate physical parameters from excitation data\n        """\n        if method == \'least_squares\':\n            return self._least_squares_identification()\n        elif method == \'gradient_descent\':\n            return self._gradient_descent_identification()\n        else:\n            raise ValueError(f"Unknown identification method: {method}")\n    \n    def _least_squares_identification(self):\n        """\n        Least squares system identification\n        """\n        # This is a simplified example - real implementation would be more complex\n        # For demonstration, we\'ll estimate simple parameters\n        \n        # Extract relevant data\n        states = np.array([d[\'state\'] for d in self.excitation_data])\n        next_states = np.array([d[\'next_state\'] for d in self.excitation_data])\n        actions = np.array([d[\'action\'] for d in self.excitation_data])\n        \n        # Approximate system dynamics: x_{t+1} = A*x_t + B*u_t + w\n        # We want to estimate A and B matrices\n        \n        # For this example, we\'ll estimate some high-level parameters\n        # like mass or friction coefficients\n        \n        # Estimate average acceleration per unit force (inverse of mass)\n        velocities = np.diff(states, axis=0)  # Approximate velocities\n        accelerations = np.diff(velocities, axis=0)  # Approximate accelerations\n        forces = actions[2:]  # Forces applied\n        \n        # Estimate mass (simplified model: F = m*a => m = F/a)\n        # For this example, we\'ll estimate an average mass-like parameter\n        avg_force = np.mean(np.abs(forces), axis=0)\n        avg_acceleration = np.mean(np.abs(accelerations), axis=0)\n        \n        # Avoid division by zero\n        estimated_mass = np.where(\n            avg_acceleration == 0, \n            1.0, \n            avg_force / avg_acceleration\n        )\n        \n        # Estimate friction (velocity damping)\n        velocity_differences = velocities[1:] - velocities[:-1]\n        avg_velocity = np.mean(np.abs(velocities[:-1]), axis=0)\n        friction_coeff = np.where(\n            avg_velocity == 0,\n            0.0,\n            np.mean(np.abs(velocity_differences), axis=0) / avg_velocity\n        )\n        \n        estimated_params = {\n            \'mass_like\': estimated_mass,\n            \'friction_like\': friction_coeff,\n            \'data_points\': len(self.excitation_data)\n        }\n        \n        print(f"Estimated parameters: {estimated_params}")\n        return estimated_params\n    \n    def _gradient_descent_identification(self):\n        """\n        Gradient descent-based system identification\n        """\n        # In a real implementation, this would train a model to match real dynamics\n        # For this example, we\'ll simulate the process\n        \n        # Initialize parameters\n        params = {\n            \'mass\': 1.0,\n            \'friction\': 0.1,\n            \'com_offset\': np.zeros(3)\n        }\n        \n        # Simulate optimization process\n        for iteration in range(100):  # 100 gradient descent steps\n            # Compute error between model prediction and real data\n            error = np.random.uniform(0.0, 1.0)  # Simulated error\n            error_grad = np.random.uniform(-0.1, 0.1, size=5)  # Simulated gradient\n            \n            # Update parameters\n            learning_rate = 0.01 * (0.99 ** iteration)  # Decaying learning rate\n            params[\'mass\'] -= learning_rate * error_grad[0]\n            params[\'friction\'] -= learning_rate * error_grad[1]\n            \n            if iteration % 20 == 0:\n                print(f"Iteration {iteration}, Error: {error:.4f}")\n        \n        print("Gradient descent identification completed")\n        return params\n\n\nclass RobustPolicy(nn.Module):\n    """\n    Robust policy that is resilient to modeling errors\n    """\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(RobustPolicy, self).__init__()\n        \n        # Actor network\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()\n        )\n        \n        # Uncertainty estimation network\n        self.uncertainty_estimator = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()  # Output uncertainty level between 0 and 1\n        )\n    \n    def forward(self, state):\n        """\n        Forward pass to get action and uncertainty\n        """\n        action = self.actor(state)\n        uncertainty = self.uncertainty_estimator(state)\n        return action, uncertainty\n    \n    def get_action_with_exploration(self, state, noise_scale=0.1):\n        """\n        Get action with added exploration noise\n        """\n        action, uncertainty = self.forward(state)\n        \n        # Scale noise based on uncertainty\n        scaled_noise = noise_scale * (1 + uncertainty) * torch.randn_like(action)\n        noisy_action = action + scaled_noise\n        \n        # Clamp to valid range (simulated)\n        max_action = 1.0  # This would be from environment\n        noisy_action = torch.clamp(noisy_action, -max_action, max_action)\n        \n        return noisy_action, uncertainty\n\n\nclass DomainAdversarialNetwork(nn.Module):\n    """\n    Network to distinguish between real and simulated data\n    for domain adaptation\n    """\n    def __init__(self, feature_dim, hidden_dim=256):\n        super(DomainAdversarialNetwork, self).__init__()\n        \n        self.domain_classifier = nn.Sequential(\n            nn.Linear(feature_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()  # Probability of being real domain\n        )\n    \n    def forward(self, features):\n        """\n        Forward pass to classify domain\n        """\n        return self.domain_classifier(features)\n\n\nclass SimToRealTransferTrainer:\n    """\n    Trainer for sim-to-real transfer with domain adaptation\n    """\n    def __init__(self, policy_network, sim_env, real_env=None):\n        self.policy_network = policy_network\n        self.sim_env = sim_env\n        self.real_env = real_env\n        self.domain_adversary = DomainAdversarialNetwork(policy_network.feature_dim)\n        \n        # Optimizers\n        self.policy_optimizer = torch.optim.Adam(policy_network.parameters(), lr=1e-4)\n        self.adversary_optimizer = torch.optim.Adam(self.domain_adversary.parameters(), lr=1e-4)\n        \n        # Loss functions\n        self.mse_loss = nn.MSELoss()\n        self.bce_loss = nn.BCELoss()\n    \n    def train_with_domain_adaptation(self, episodes=1000, batch_size=64):\n        """\n        Train policy with domain adaptation\n        """\n        print("Starting training with domain adaptation...")\n        \n        for episode in range(episodes):\n            # Collect simulated data\n            sim_episode_data = self.collect_episode_data(self.sim_env, domain=\'sim\')\n            \n            # Collect real data if available\n            real_episode_data = None\n            if self.real_env:\n                real_episode_data = self.collect_episode_data(self.real_env, domain=\'real\')\n            \n            # Domain adaptation training\n            if real_episode_data:\n                self.train_adversarial_step(sim_episode_data, real_episode_data)\n            \n            # Policy training\n            self.train_policy_step(sim_episode_data)\n            \n            if episode % 100 == 0:\n                print(f"Episode {episode}, training progress...")\n    \n    def collect_episode_data(self, env, domain):\n        """\n        Collect data from an episode\n        """\n        state = env.reset()\n        episode_data = []\n        \n        for step in range(100):  # Max 100 steps per episode\n            state_tensor = torch.FloatTensor(state)\n            action, uncertainty = self.policy_network(state_tensor)\n            \n            # Execute action in environment\n            next_state, reward, done, info = env.step(action.detach().numpy())\n            \n            episode_data.append({\n                \'state\': state,\n                \'action\': action.detach().numpy(),\n                \'reward\': reward,\n                \'next_state\': next_state,\n                \'done\': done,\n                \'domain\': domain,\n                \'uncertainty\': uncertainty.item() if uncertainty.numel() == 1 else uncertainty.detach().numpy()\n            })\n            \n            state = next_state\n            \n            if done:\n                break\n        \n        return episode_data\n    \n    def train_adversarial_step(self, sim_data, real_data):\n        """\n        Train the domain adversarial component\n        """\n        # Prepare data\n        sim_states = torch.stack([torch.FloatTensor(d[\'state\']) for d in sim_data])\n        real_states = torch.stack([torch.FloatTensor(d[\'state\']) for d in real_data])\n        \n        # Label: 0 for simulated, 1 for real\n        sim_labels = torch.zeros(len(sim_data), 1)\n        real_labels = torch.ones(len(real_data), 1)\n        \n        # Concatenate data\n        all_states = torch.cat([sim_states, real_states], dim=0)\n        all_labels = torch.cat([sim_labels, real_labels], dim=0)\n        \n        # Train domain classifier (adversary)\n        self.adversary_optimizer.zero_grad()\n        \n        domain_predictions = self.domain_adversary(all_states)\n        adversary_loss = self.bce_loss(domain_predictions, all_labels)\n        \n        adversary_loss.backward()\n        self.adversary_optimizer.step()\n        \n        # Train policy to fool domain classifier (feature alignment)\n        self.policy_optimizer.zero_grad()\n        \n        sim_predictions = self.domain_adversary(sim_states)\n        # Try to make simulated data look real (label = 1)\n        policy_adv_loss = self.bce_loss(sim_predictions, torch.ones_like(sim_predictions))\n        \n        policy_adv_loss.backward()\n        self.policy_optimizer.step()\n    \n    def train_policy_step(self, episode_data):\n        """\n        Train the policy network\n        """\n        # This would implement the policy optimization step\n        # For this example, we\'ll just pass (implementation would depend on specific RL algorithm)\n        pass\n\n\ndef visualize_randomization_effects():\n    """\n    Visualize the effects of domain randomization\n    """\n    print("Visualizing domain randomization effects...")\n    \n    # Example visualization of how randomization affects policy behavior\n    base_action = np.array([0.5, -0.3, 0.8])  # Base action\n    \n    # Different randomization parameters\n    mass_multipliers = [0.8, 1.0, 1.2]\n    friction_multipliers = [0.5, 1.0, 1.5]\n    \n    print("Effect of different parameter multipliers on action effectiveness:")\n    for mass_mult in mass_multipliers:\n        for fric_mult in friction_multipliers:\n            # Simulate how different parameters would affect action outcome\n            # This is a simplified model - real physics would be more complex\n            effective_action = base_action / (mass_mult * fric_mult)\n            print(f"  Mass: {mass_mult:.1f}, Friction: {fric_mult:.1f} -> Effective action: {effective_action}")\n\n\ndef main():\n    """\n    Example usage of sim-to-real transfer techniques\n    """\n    print("Sim-to-Real Transfer Techniques")\n    print("=" * 50)\n    \n    # Example 1: Domain Randomization\n    print("\\n1. Domain Randomization Example")\n    \n    # Define randomization configuration\n    randomization_config = {\n        \'mass_range\': [0.8, 1.2],\n        \'friction_range\': [0.7, 1.3],\n        \'com_range\': [-0.05, 0.05],\n        \'actuator_range\': [0.9, 1.1],\n        \'sensor_noise_range\': [0.0, 0.05]\n    }\n    \n    print(f"Randomization parameters: {randomization_config}")\n    \n    # Example 2: System Identification\n    print("\\n2. System Identification Example")\n    \n    # This would connect to a real or simulated robot\n    # For demonstration, we\'ll create a mock robot model\n    class MockRobotModel:\n        def __init__(self):\n            self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(3,), dtype=np.float32)\n        \n        def get_state(self):\n            return np.random.uniform(-1, 1, size=12)  # 12-dimensional state\n        \n        def step(self, action):\n            next_state = np.random.uniform(-1, 1, size=12)\n            reward = np.random.uniform(-1, 1)\n            done = random.random() < 0.01  # 1% chance of done\n            info = {}\n            return next_state, reward, done, info\n    \n    robot_model = MockRobotModel()\n    system_id = SystemID(robot_model)\n    \n    # Collect data and estimate parameters\n    system_id.collect_excitation_data(duration=5.0)\n    estimated_params = system_id.estimate_parameters()\n    print(f"Estimated parameters: {estimated_params}")\n    \n    # Example 3: Robust Policy\n    print("\\n3. Robust Policy Example")\n    \n    state_dim = 12  # Example state dimension\n    action_dim = 3  # Example action dimension\n    \n    robust_policy = RobustPolicy(state_dim, action_dim)\n    print(f"Robust policy initialized with {state_dim} states and {action_dim} actions")\n    \n    # Test the policy\n    test_state = torch.randn(1, state_dim)\n    action, uncertainty = robust_policy(test_state)\n    print(f"Action: {action.detach().numpy()}, Uncertainty: {uncertainty.detach().numpy()}")\n    \n    # Example 4: Visualization\n    print("\\n4. Randomization Effects Visualization")\n    visualize_randomization_effects()\n    \n    print("\\n5. Best Practices for Sim-to-Real Transfer:")\n    print("  - Use domain randomization to improve generalization")\n    print("  - Collect rich excitation data for system identification")\n    print("  - Implement robust control strategies")\n    print("  - Validate policies in simulation before real-world testing")\n    print("  - Monitor safety during real-world deployment")\n    \n    print("\\nThis example demonstrates key techniques for successful")\n    print("sim-to-real transfer in robotics applications.")\n\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"simulation-demonstration",children:"Simulation Demonstration"}),"\n",(0,t.jsx)(e.p,{children:"This implementation demonstrates key techniques for sim-to-real transfer in robotics, including domain randomization, system identification, and robust control strategies. The code provides a framework for randomizing simulation parameters to improve policy generalization and includes methods for identifying real-world system parameters. The techniques can be applied to any robotic platform with appropriate modifications."}),"\n",(0,t.jsx)(e.h2,{id:"hands-on-lab-sim-to-real-transfer-implementation",children:"Hands-On Lab: Sim-to-Real Transfer Implementation"}),"\n",(0,t.jsx)(e.p,{children:"In this lab, you'll implement and test sim-to-real transfer techniques:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Implement domain randomization for a robotic task"}),"\n",(0,t.jsx)(e.li,{children:"Perform system identification on a simulated robot"}),"\n",(0,t.jsx)(e.li,{children:"Train a robust policy with randomized parameters"}),"\n",(0,t.jsx)(e.li,{children:"Test the transfer to a less randomized environment"}),"\n",(0,t.jsx)(e.li,{children:"Evaluate the effectiveness of different transfer techniques"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"required-equipment",children:"Required Equipment:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Python development environment"}),"\n",(0,t.jsx)(e.li,{children:"PyTorch or TensorFlow"}),"\n",(0,t.jsx)(e.li,{children:"Robot simulation environment (Gazebo, Isaac Sim, PyBullet)"}),"\n",(0,t.jsx)(e.li,{children:"(Optional) Physical robot for validation"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"instructions",children:"Instructions:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Create a new Python package for sim-to-real experiments"}),"\n",(0,t.jsx)(e.li,{children:"Implement the DomainRandomizationWrapper class"}),"\n",(0,t.jsx)(e.li,{children:"Create a robotic task environment (e.g., reaching, navigation)"}),"\n",(0,t.jsx)(e.li,{children:"Train a policy with domain randomization"}),"\n",(0,t.jsx)(e.li,{children:"Test the policy with varying levels of randomization"}),"\n",(0,t.jsx)(e.li,{children:"Implement system identification techniques"}),"\n",(0,t.jsx)(e.li,{children:"Compare policies trained with and without domain randomization"}),"\n",(0,t.jsx)(e.li,{children:"Document the transfer performance improvements"}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"common-pitfalls--debugging-notes",children:"Common Pitfalls & Debugging Notes"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Over-Randomization"}),": Excessive randomization can lead to poor performance"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Under-Randomization"}),": Insufficient randomization fails to improve transfer"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Real-World Validation"}),": Always test on real hardware before deployment"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Concerns"}),": Monitor for unsafe behaviors during transfer"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Computational Cost"}),": Domain randomization can significantly increase training time"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Parameter Selection"}),": Choosing appropriate randomization ranges requires domain knowledge"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Model Fidelity"}),": Very simple simulators may not benefit from complex randomization"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"summary--key-terms",children:"Summary & Key Terms"}),"\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Key Terms:"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reality Gap"}),": Differences between simulation and real robot behavior"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Domain Randomization"}),": Technique to improve policy generalization"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"System Identification"}),": Process of finding accurate system models"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Transfer Learning"}),": Applying knowledge from one domain to another"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Domain Adaptation"}),": Adapting models to new domains"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robust Control"}),": Control strategies resilient to model errors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Sim-to-Real Transfer"}),": Application of simulated policies to real robots"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"further-reading--citations",children:"Further Reading & Citations"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:'Sadeghi, F., & Levine, S. (2017). "CAD2RL: Real single-image flight without a single real image." arXiv preprint arXiv:1611.04208.'}),"\n",(0,t.jsx)(e.li,{children:'Tobin, J., et al. (2017). "Domain randomization for transferring deep neural networks from simulation to the real world." IEEE/RSJ International Conference on Intelligent Robots and Systems.'}),"\n",(0,t.jsx)(e.li,{children:'Peng, X. B., et al. (2018). "Sim-to-real transfer of robotic control with dynamics randomization." IEEE International Conference on Robotics and Automation.'}),"\n",(0,t.jsx)(e.li,{children:'James, S., et al. (2019). "Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation policies." IEEE/CVF Conference on Computer Vision and Pattern Recognition.'}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"assessment-questions",children:"Assessment Questions"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Explain the concept of domain randomization and its role in sim-to-real transfer."}),"\n",(0,t.jsx)(e.li,{children:"What are the main causes of the reality gap between simulation and reality?"}),"\n",(0,t.jsx)(e.li,{children:"Describe how system identification can improve sim-to-real transfer."}),"\n",(0,t.jsx)(e.li,{children:"What safety considerations are important when transferring policies to real robots?"}),"\n",(0,t.jsx)(e.li,{children:"How would you evaluate the success of a sim-to-real transfer approach?"}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Previous"}),": ",(0,t.jsx)(e.a,{href:"/docs/appendices/reinforcement-learning",children:"Reinforcement Learning for Robot Control"}),(0,t.jsx)(e.br,{}),"\n",(0,t.jsx)(e.strong,{children:"Next"}),": ",(0,t.jsx)(e.a,{href:"/docs/meta/glossary",children:"Glossary of Robotics Terms"})]})]})}function m(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>s});var a=i(6540);const t={},r=a.createContext(t);function o(n){const e=a.useContext(r);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:o(n.components),a.createElement(r.Provider,{value:e},n.children)}}}]);